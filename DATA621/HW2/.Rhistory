str(df)
seq(0,1,0.1)
# 10.
plot_ROC = function(df) {
true_positive_rate = rep(0, 100)
false_positive_rate = rep(0, 100)
for (i in seq(0, 1, 0.01)) {
df$scored.class = ifelse(df$scored.probability > i, 1, 0)
true_positive_rate[i] = pred_sensitivity(df)
false_positive_rate[i] = 1 - pred_specificity(df)
}
plot(x = false_positive_rate, y = true_positive_rate)
}
plot_ROC(df)
i = 0
df$scored.class = ifelse(df$scored.probability > i, 1, 0)
true_positive_rate = rep(0, 100)
false_positive_rate = rep(0, 100)
true_positive_rate[i] = pred_sensitivity(df)
false_positive_rate[i] = 1 - pred_specificity(df)
true_positive_rate
str(true_positive_rate)
true_positive_rate = rep(0, 100)
str(true_positive_rate)
# 10.
plot_ROC = function(df, scale = 100) {
true_positive_rate = rep(0, scale)
false_positive_rate = rep(0, scale)
for (i in seq(0, 1, scale / 100)) {
df$scored.class = ifelse(df$scored.probability > i, 1, 0)
true_positive_rate[i*scale] = pred_sensitivity(df)
false_positive_rate[i*scale] = 1 - pred_specificity(df)
}
plot(x = false_positive_rate, y = true_positive_rate)
}
plot_ROC(df)
true_positive_rate = rep(0, scale)
false_positive_rate = rep(0, scale)
scale = 100
true_positive_rate = rep(0, scale)
false_positive_rate = rep(0, scale)
df$scored.class = ifelse(df$scored.probability > i, 1, 0)
true_positive_rate[i*scale] = pred_sensitivity(df)
false_positive_rate[i*scale] = 1 - pred_specificity(df)
true_positive_rate
pred_sensitivity(df)
as.integer(pred_sensitivity(df))
as.numeric(pred_sensitivity(df))
# 10.
plot_ROC = function(df, scale = 100) {
true_positive_rate = rep(0, scale)
false_positive_rate = rep(0, scale)
for (i in seq(0, 1, scale / 100)) {
df$scored.class = ifelse(df$scored.probability > i, 1, 0)
true_positive_rate[i*scale] = as.numeric(pred_sensitivity(df))
false_positive_rate[i*scale] = 1 - as.numeric(pred_specificity(df))
}
plot(x = false_positive_rate, y = true_positive_rate)
}
plot_ROC(df)
true_positive_rate = rep(0, scale)
false_positive_rate = rep(0, scale)
true_positive_rate[i*scale] = as.numeric(pred_sensitivity(df))
true_positive_rate
false_positive_rate[i*scale] = 1 - as.numeric(pred_specificity(df))
false_positive_rate
as.numeric(pred_sensitivity(df))
true_positive_rate[i*scale]
true_positive_rate[i*scale+1] = as.numeric(pred_sensitivity(df))
true_positive_rate
false_positive_rate
# 10.
plot_ROC = function(df, scale = 100) {
true_positive_rate = rep(0, scale+1)
false_positive_rate = rep(0, scale+1)
for (i in seq(0, 1, scale / 100)) {
df$scored.class = ifelse(df$scored.probability > i, 1, 0)
true_positive_rate[i*scale+1] = as.numeric(pred_sensitivity(df))
false_positive_rate[i*scale+1] = 1 - as.numeric(pred_specificity(df))
}
plot(x = false_positive_rate, y = true_positive_rate)
}
plot_ROC(df)
true_positive_rate
i = 0.01
df$scored.class = ifelse(df$scored.probability > i, 1, 0)
true_positive_rate[i*scale+1] = as.numeric(pred_sensitivity(df))
false_positive_rate[i*scale+1] = 1 - as.numeric(pred_specificity(df))
true_positive_rate
false_positive_rate
as.numeric(pred_specificity(df))
as.numeric(pred_sensitivity(df))
df[, c('scored.class', 'scored.probability')]
df %>% filter(class != 1 & scored.class != 1) %>% summarise(n = n())
df %>% filter(class != 1 & scored.class != 1) %>% summarise(n = n())
# 3-8.
# accuracy = % of true positives and true negatives
pred_accuracy = function(df) {
df %>% filter(class == scored.class) %>% summarise(n = n()) /
nrow(df)
}
# error rate = % of false positives and false negatives
pred_error_rate = function(df) {
df %>% filter(class != scored.class) %>% summarise(n = n()) /
nrow(df)
}
# precision = % of all positive predictions that were actually positives
pred_precision = function(df) {
df %>% filter(class == 1 & scored.class == 1) %>% summarise(n = n()) /
(df %>% filter(class == 1 & scored.class == 1) %>% summarise(n = n()) +
df %>% filter(class != 1 & scored.class == 1) %>% summarise(n = n()))
}
# sensitivity/recall = % of all actual positives that were predicted positives
pred_sensitivity = function(df) {
df %>% filter(class == 1 & scored.class == 1) %>% summarise(n = n()) /
(df %>% filter(class == 1 & scored.class == 1) %>% summarise(n = n()) +
df %>% filter(class == 1 & scored.class != 1) %>% summarise(n = n()))
}
# specificity = % of all actual negatives that were predicted as negatives
pred_specificity = function(df) {
df %>% filter(class != 1 & scored.class != 1) %>% summarise(n = n()) /
(df %>% filter(class != 1 & scored.class != 1) %>% summarise(n = n()) +
df %>% filter(class != 1 & scored.class == 1) %>% summarise(n = n()))
}
# F1 score
pred_F1 = function(df) {
(2 * pred_precision(df) * pred_sensitivity(df)) /
(pred_precision(df) + pred_sensitivity(df))
}
as.numeric(pred_specificity(df))
# 3-8.
# accuracy = % of true positives and true negatives
pred_accuracy = function(df) {
df %>% filter(class == scored.class) %>% summarise(n = n()) /
nrow(df)
}
# error rate = % of false positives and false negatives
pred_error_rate = function(df) {
df %>% filter(class != scored.class) %>% summarise(n = n()) /
nrow(df)
}
# precision = % of all positive predictions that were actually positives
pred_precision = function(df) {
df %>% filter(class == 1 & scored.class == 1) %>% summarise(n = n()) /
(df %>% filter(class == 1 & scored.class == 1) %>% summarise(n = n()) +
df %>% filter(class != 1 & scored.class == 1) %>% summarise(n = n()))
}
# sensitivity/recall = % of all actual positives that were predicted positives
pred_sensitivity = function(df) {
df %>% filter(class == 1 & scored.class == 1) %>% summarise(n = n()) /
(df %>% filter(class == 1 & scored.class == 1) %>% summarise(n = n()) +
df %>% filter(class == 1 & scored.class != 1) %>% summarise(n = n()))
}
# specificity = % of all actual negatives that were predicted as negatives
pred_specificity = function(df) {
df %>% filter(class != 1 & scored.class != 1) %>% summarise(n = n()) /
(df %>% filter(class != 1) %>% summarise(n = n())
}
# 3-8.
# accuracy = % of true positives and true negatives
pred_accuracy = function(df) {
df %>% filter(class == scored.class) %>% summarise(n = n()) /
nrow(df)
}
# error rate = % of false positives and false negatives
pred_error_rate = function(df) {
df %>% filter(class != scored.class) %>% summarise(n = n()) /
nrow(df)
}
# precision = % of all positive predictions that were actually positives
pred_precision = function(df) {
df %>% filter(class == 1 & scored.class == 1) %>% summarise(n = n()) /
(df %>% filter(class == 1 & scored.class == 1) %>% summarise(n = n()) +
df %>% filter(class != 1 & scored.class == 1) %>% summarise(n = n()))
}
# sensitivity/recall = % of all actual positives that were predicted positives
pred_sensitivity = function(df) {
df %>% filter(class == 1 & scored.class == 1) %>% summarise(n = n()) /
(df %>% filter(class == 1 & scored.class == 1) %>% summarise(n = n()) +
df %>% filter(class == 1 & scored.class != 1) %>% summarise(n = n()))
}
# specificity = % of all actual negatives that were predicted as negatives
pred_specificity = function(df) {
df %>% filter(class != 1 & scored.class != 1) %>% summarise(n = n()) /
df %>% filter(class != 1) %>% summarise(n = n())
}
# F1 score
pred_F1 = function(df) {
(2 * pred_precision(df) * pred_sensitivity(df)) /
(pred_precision(df) + pred_sensitivity(df))
}
false_positive_rate[i*scale+1] = 1 - as.numeric(pred_specificity(df))
as.numeric(pred_specificity(df))
1 - as.numeric(pred_specificity(df))
# 3-8.
# accuracy = % of true positives and true negatives
pred_accuracy = function(df) {
df %>% filter(class == scored.class) %>% summarise(n = n()) /
nrow(df)
}
# error rate = % of false positives and false negatives
pred_error_rate = function(df) {
df %>% filter(class != scored.class) %>% summarise(n = n()) /
nrow(df)
}
# precision = % of all positive predictions that were actually positives
pred_precision = function(df) {
df %>% filter(class == 1 & scored.class == 1) %>% summarise(n = n()) /
(df %>% filter(class == 1 & scored.class == 1) %>% summarise(n = n()) +
df %>% filter(class != 1 & scored.class == 1) %>% summarise(n = n()))
}
# sensitivity/recall = % of all actual positives that were predicted positives
pred_sensitivity = function(df) {
df %>% filter(class == 1 & scored.class == 1) %>% summarise(n = n()) /
(df %>% filter(class == 1 & scored.class == 1) %>% summarise(n = n()) +
df %>% filter(class == 1 & scored.class != 1) %>% summarise(n = n()))
}
# specificity = % of all actual negatives that were predicted as negatives
pred_specificity = function(df) {
df %>% filter(class == 0 & scored.class == 0) %>% summarise(n = n()) /
df %>% filter(class == 0) %>% summarise(n = n())
}
# F1 score
pred_F1 = function(df) {
(2 * pred_precision(df) * pred_sensitivity(df)) /
(pred_precision(df) + pred_sensitivity(df))
}
as.numeric(pred_specificity(df))
# 10.
plot_ROC = function(df, scale = 100) {
true_positive_rate = rep(0, scale+1)
false_positive_rate = rep(0, scale+1)
for (i in seq(0, 1, scale / 100)) {
df$scored.class = ifelse(df$scored.probability > i, 1, 0)
true_positive_rate[i*scale+1] = as.numeric(pred_sensitivity(df))
false_positive_rate[i*scale+1] = 1 - as.numeric(pred_specificity(df))
}
plot(x = false_positive_rate, y = true_positive_rate)
}
plot_ROC(df)
i = 0.9
df$scored.class = ifelse(df$scored.probability > i, 1, 0)
true_positive_rate[i*scale+1] = as.numeric(pred_sensitivity(df))
false_positive_rate[i*scale+1] = 1 - as.numeric(pred_specificity(df))
as.numeric(pred_sensitivity(df))
as.numeric(pred_specificity(df))
i = 0
df$scored.class = ifelse(df$scored.probability > i, 1, 0)
as.numeric(pred_specificity(df))
# 10.
plot_ROC = function(df, scale = 100) {
true_positive_rate = rep(0, scale+1)
false_positive_rate = rep(0, scale+1)
for (i in seq(0, 1, scale / 100)) {
df$scored.class = ifelse(df$scored.probability > i, 1, 0)
true_positive_rate[i*scale+1] = as.numeric(pred_sensitivity(df))
false_positive_rate[i*scale+1] = 1 - as.numeric(pred_specificity(df))
}
plot(x = false_positive_rate, y = true_positive_rate)
}
plot_ROC(df)
true_positive_rate
false_positive_rate
plot(x = false_positive_rate, y = true_positive_rate)
# 10.
plot_ROC = function(df, scale = 100) {
true_positive_rate = rep(0, scale+1)
false_positive_rate = rep(0, scale+1)
for (i in seq(0, 1, scale / 100)) {
df$scored.class = ifelse(df$scored.probability > i, 1, 0)
true_positive_rate[i*scale+1] = as.numeric(pred_sensitivity(df))
false_positive_rate[i*scale+1] = 1 - as.numeric(pred_specificity(df))
}
plot(x = false_positive_rate, y = true_positive_rate)
}
plot_ROC(df)
for (i in seq(0, 1, scale / 100)) { print (i) }
seq(0, 1, scale / 100))
seq(0, 1, scale / 100)
# 10.
plot_ROC = function(df, scale = 100) {
true_positive_rate = rep(0, scale+1)
false_positive_rate = rep(0, scale+1)
for (i in seq(0, 1, 1 / scale)) {
df$scored.class = ifelse(df$scored.probability > i, 1, 0)
true_positive_rate[i*scale+1] = as.numeric(pred_sensitivity(df))
false_positive_rate[i*scale+1] = 1 - as.numeric(pred_specificity(df))
}
plot(x = false_positive_rate, y = true_positive_rate)
}
plot_ROC(df)
# 10.
plot_ROC = function(df, scale = 100) {
true_positive_rate = rep(0, scale+1)
false_positive_rate = rep(0, scale+1)
for (i in seq(0, 1, 1 / scale)) {
df$scored.class = ifelse(df$scored.probability > i, 1, 0)
true_positive_rate[i*scale+1] = as.numeric(pred_sensitivity(df))
false_positive_rate[i*scale+1] = 1 - as.numeric(pred_specificity(df))
}
# plot(x = false_positive_rate, y = true_positive_rate)
df %>% ggplot(aes(x = false_positive_rate, y = true_positive_rate)) +
geom_point() + geom_smooth()
}
plot_ROC(df)
library(ggplot2)
# 10.
plot_ROC = function(df, scale = 100) {
require(ggplot2)
true_positive_rate = rep(0, scale+1)
false_positive_rate = rep(0, scale+1)
for (i in seq(0, 1, 1 / scale)) {
df$scored.class = ifelse(df$scored.probability > i, 1, 0)
true_positive_rate[i*scale+1] = as.numeric(pred_sensitivity(df))
false_positive_rate[i*scale+1] = 1 - as.numeric(pred_specificity(df))
}
# plot(x = false_positive_rate, y = true_positive_rate)
df %>% ggplot(aes(x = false_positive_rate, y = true_positive_rate)) +
geom_point() + geom_smooth()
}
plot_ROC(df)
# 10.
plot_ROC = function(df, scale = 100) {
require(ggplot2)
true_positive_rate = rep(0, scale+1)
false_positive_rate = rep(0, scale+1)
for (i in seq(0, 1, 1 / scale)) {
df$scored.class = ifelse(df$scored.probability > i, 1, 0)
true_positive_rate[i*scale+1] = as.numeric(pred_sensitivity(df))
false_positive_rate[i*scale+1] = 1 - as.numeric(pred_specificity(df))
}
# plot(x = false_positive_rate, y = true_positive_rate)
return(df)
}
plot_ROC(df)
temp = plot_ROC(df)
temp %>% ggplot(aes(x = false_positive_rate, y = true_positive_rate)) + geom_point()
temp %>% ggplot(aes(x = false_positive_rate, y = true_positive_rate)) + geom_line()
str(temp)
# 10.
plot_ROC = function(df, scale = 100) {
require(ggplot2)
true_positive_rate = rep(0, scale+1)
false_positive_rate = rep(0, scale+1)
for (i in seq(0, 1, 1 / scale)) {
df$scored.class = ifelse(df$scored.probability > i, 1, 0)
true_positive_rate[i*scale+1] = as.numeric(pred_sensitivity(df))
false_positive_rate[i*scale+1] = 1 - as.numeric(pred_specificity(df))
}
# plot(x = false_positive_rate, y = true_positive_rate)
return(data.frame(x = false_positive_rate, y = true_positive_rate))
}
temp = plot_ROC(df)
temp %>% ggplot(aes(x = false_positive_rate, y = true_positive_rate)) + geom_point()
str(temp)
plot(temp$x, temp$y)
temp %>% ggplot(aes(x = false_positive_rate, y = true_positive_rate)) + geom_point()
temp %>% ggplot(aes(x = false_positive_rate, y = true_positive_rate)) + geom_point(size = 2)
length(temp$x)
length(temp$y)
temp %>% ggplot() + geom_point(aes(x =false_positive_rate, y = true_positive_rate))
View(temp)
ggplot(temp, aes(x = x, y = y)) + geom_point()
temp %>% ggplot(aes(x = x, y = y)) + geom_point(size = 2)
temp %>% ggplot(aes(x = x, y = y)) + geom_point(size = 2) + geom_smooth()
temp %>% ggplot(aes(x = x, y = y)) + geom_point(size = 2) + geom_line()
temp %>% ggplot(aes(x = x, y = y)) + geom_point(size = 2) + geom_smooth(se = FALSE)
temp %>% ggplot(aes(x = x, y = y)) + geom_point(size = 2)
ggplot(aes(x = temp$x, y = temp$y)) + geom_point(size = 2)
plot(x = false_positive_rate, y = true_positive_rate)
plot_ROC = function(df, scale = 100) {
require(ggplot2)
true_positive_rate = rep(0, scale+1)
false_positive_rate = rep(0, scale+1)
for (i in seq(0, 1, 1 / scale)) {
df$scored.class = ifelse(df$scored.probability > i, 1, 0)
true_positive_rate[i*scale+1] = as.numeric(pred_sensitivity(df))
false_positive_rate[i*scale+1] = 1 - as.numeric(pred_specificity(df))
}
# plot(x = false_positive_rate, y = true_positive_rate)
data.frame(false_positive_rate = false_positive_rate, true_positive_rate = true_positive_rate) %>%
ggplot(aes(x = false_positive_rate, y = true_positive_rate)) + geom_point()
}
plot_ROC(df)
# 10.
plot_ROC = function(df, scale = 100) {
require(ggplot2)
true_positive_rate = rep(0, scale+1)
false_positive_rate = rep(0, scale+1)
for (i in seq(0, 1, 1 / scale)) {
df$scored.class = ifelse(df$scored.probability > i, 1, 0)
true_positive_rate[i*scale+1] = as.numeric(pred_sensitivity(df))
false_positive_rate[i*scale+1] = 1 - as.numeric(pred_specificity(df))
}
# plot(x = false_positive_rate, y = true_positive_rate)
data.frame(false_positive_rate = false_positive_rate, true_positive_rate = true_positive_rate) %>%
ggplot(aes(x = false_positive_rate, y = true_positive_rate)) + geom_point() +
ggtitle('ROC Curve')
}
plot_ROC(df)
# 3-8.
# accuracy = % of true positives and true negatives
pred_accuracy = function(df) {
df %>% filter(class == scored.class) %>% summarise(n = n()) /
nrow(df)
}
# error rate = % of false positives and false negatives
pred_error_rate = function(df) {
df %>% filter(class != scored.class) %>% summarise(n = n()) /
nrow(df)
}
# precision = % of all positive predictions that were actually positives
pred_precision = function(df) {
df %>% filter(class == 1 & scored.class == 1) %>% summarise(n = n()) /
df %>% filter(scored.class == 1) %>% summarise(n = n())
}
# sensitivity/recall = % of all actual positives that were predicted positives
pred_sensitivity = function(df) {
df %>% filter(class == 1 & scored.class == 1) %>% summarise(n = n()) /
df %>% filter(class == 1) %>% summarise(n = n())
}
# specificity = % of all actual negatives that were predicted as negatives
pred_specificity = function(df) {
df %>% filter(class == 0 & scored.class == 0) %>% summarise(n = n()) /
df %>% filter(class == 0) %>% summarise(n = n())
}
# F1 score
pred_F1 = function(df) {
(2 * pred_precision(df) * pred_sensitivity(df)) /
(pred_precision(df) + pred_sensitivity(df))
}
# 10.
plot_ROC = function(df, scale = 100) {
require(ggplot2)
true_positive_rate = rep(0, scale+1)
false_positive_rate = rep(0, scale+1)
for (i in seq(0, 1, 1 / scale)) {
df$scored.class = ifelse(df$scored.probability > i, 1, 0)
true_positive_rate[i*scale+1] = as.numeric(pred_sensitivity(df))
false_positive_rate[i*scale+1] = 1 - as.numeric(pred_specificity(df))
}
# plot(x = false_positive_rate, y = true_positive_rate)
data.frame(false_positive_rate = false_positive_rate, true_positive_rate = true_positive_rate) %>%
ggplot(aes(x = false_positive_rate, y = true_positive_rate)) + geom_point() +
ggtitle('ROC Curve')
}
plot_ROC(df)
# 11.
# accuracy = % of true positives and true negatives
pred_accuracy(df)
# error rate = % of false positives and false negatives
pred_error_rate(df)
# precision = % of all positive predictions that were actually positives
pred_precision(df)
# sensitivity/recall = % of all actual positives that were predicted positives
pred_sensitivity(df)
# specificity = % of all actual negatives that were predicted as negatives
pred_specificity(df)
# F1 score
pred_F1(df)
# 1-2.
setwd("~/Google Drive/CUNY/git/DATA621/HW2")
library(dplyr)
library(ggplot2)
df = read.csv('classification-output-data.csv', header = TRUE)
str(df)
table(pred = df$scored.class, actual = df$class)
# 11.
# accuracy = % of true positives and true negatives
pred_accuracy(df)
# error rate = % of false positives and false negatives
pred_error_rate(df)
# precision = % of all positive predictions that were actually positives
pred_precision(df)
# sensitivity/recall = % of all actual positives that were predicted positives
pred_sensitivity(df)
# specificity = % of all actual negatives that were predicted as negatives
pred_specificity(df)
# F1 score
pred_F1(df)
library(caret)
confusionMatrix(df$scored.class, df$class)
?confusionMatrix
pred_sensitivity = function(df) {
df %>% filter(class == 1 & scored.class == 1) %>% summarise(n = n()) /
df %>% filter(class == 1) %>% summarise(n = n())
}
pred_sensitivity(df)
pred_sensitivity = function(df) {
df %>% filter(class == 1 & scored.class == 1) %>% summarise(n = n()) /
(df %>% filter(class == 1 & scored.class == 1) %>% summarise(n = n()) +
df %>% filter(class == 1 & scored.class == 0) %>% summarise(n = n()))
}
pred_sensitivity(df)
pred_sensitivity(df)
confusionMatrix(df$class, df$scored.class)
confusionMatrix(df$scored.class, df$class)
confusionMatrix(df$scored.class, df$class, positive = 1)
confusionMatrix(df$scored.class, df$class, positive = '1')
confusionMatrix(df$scored.class, df$class)
confusionMatrix(df$scored.class, df$class, positive = '1')
confusionMatrix(df$scored.class, df$class, positive = '0')
confusionMatrix(df$scored.class, df$class, positive = '1')
install.packages("pROC")
?pROC
roc(df$class, df$scored.class)
library(pROC)
roc(df$class, df$scored.class)
plot.roc(df$class, df$scored.class)
plot.roc(df$class, df$scored.probability)
library(caTools)
colAUC(df$scored.probability, df$class, plotROC = TRUE)
