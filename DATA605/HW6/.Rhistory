source('G:/INVESTMENT TEAM/Invest Statements/Internal Reporting/R/ReturnAnalysis_FundDirectSAC.R', echo=TRUE)
str(qry)
qry
dbhandle <- odbcDriverConnect('driver={SQL Server};server=sql01.capricornllc.net;
database=CapricornDB;trusted_connection=true')
qry = sqlQuery(dbhandle, "
SELECT 'Sequoia Portfolio' AS 'LegalEntityName', 'Natural Resources' AS 'SAC', 'Fund' AS 'FundDirect', 'Actual' AS 'ActualPolicy', EOMONTH(PeriodDate,0) AS 'PeriodDate', [Return],
(SELECT SUM(EndingValue) FROM v_LegalEntityDealValue t
WHERE LegalEntityName = 'Sequoia Portfolio' AND t.PeriodDate = a.PeriodDate AND t.SecondaryAssetClass = 'Natural Resources' AND t.FundDirect = 'Fund') AS EndingValue,
(SELECT SUM(EndingValue) FROM v_LegalEntityDealValue t
WHERE LegalEntityName = 'Sequoia Portfolio' AND t.PeriodDate = a.PeriodDate) AS PortNAV
FROM dbo.udf_CombinedConsolidatedReturns(96, NULL, 'Natural Resources', 'Fund', NULL, '01/01/2011', '11/30/2016') a
UNION ALL
SELECT 'Sequoia Portfolio' AS 'LegalEntityName', 'Natural Resources' AS 'SAC', 'Direct' AS 'FundDirect', 'Actual' AS 'ActualPolicy', EOMONTH(PeriodDate,0) AS 'PeriodDate', [Return],
(SELECT SUM(EndingValue) FROM v_LegalEntityDealValue t
WHERE LegalEntityName = 'Sequoia Portfolio' AND t.PeriodDate = a.PeriodDate AND t.SecondaryAssetClass = 'Natural Resources' AND t.FundDirect = 'Direct') AS EndingValue,
(SELECT SUM(EndingValue) FROM v_LegalEntityDealValue t
WHERE LegalEntityName = 'Sequoia Portfolio' AND t.PeriodDate = a.PeriodDate) AS PortNAV
FROM dbo.udf_CombinedConsolidatedReturns(96, NULL, 'Natural Resources', 'Direct', NULL, '01/01/2011', '11/30/2016') a
UNION ALL
SELECT 'Sequoia Portfolio' AS 'LegalEntityName', 'Real Estate & Infrastructure' AS 'SAC', 'Fund' AS 'FundDirect', 'Actual' AS 'ActualPolicy', EOMONTH(PeriodDate,0) AS 'PeriodDate', [Return],
(SELECT SUM(EndingValue) FROM v_LegalEntityDealValue t
WHERE LegalEntityName = 'Sequoia Portfolio' AND t.PeriodDate = a.PeriodDate AND t.SecondaryAssetClass = 'Real Estate & Infrastructure' AND t.FundDirect = 'Fund') AS EndingValue,
(SELECT SUM(EndingValue) FROM v_LegalEntityDealValue t
WHERE LegalEntityName = 'Sequoia Portfolio' AND t.PeriodDate = a.PeriodDate) AS PortNAV
FROM dbo.udf_CombinedConsolidatedReturns(96, NULL, 'Real Estate & Infrastructure', 'Fund', NULL, '01/01/2011', '11/30/2016') a
UNION ALL
SELECT 'Sequoia Portfolio' AS 'LegalEntityName', 'Real Estate & Infrastructure' AS 'SAC', 'Direct' AS 'FundDirect', 'Actual' AS 'ActualPolicy', EOMONTH(PeriodDate,0) AS 'PeriodDate', [Return],
(SELECT SUM(EndingValue) FROM v_LegalEntityDealValue t
WHERE LegalEntityName = 'Sequoia Portfolio' AND t.PeriodDate = a.PeriodDate AND t.SecondaryAssetClass = 'Real Estate & Infrastructure' AND t.FundDirect = 'Direct') AS EndingValue,
(SELECT SUM(EndingValue) FROM v_LegalEntityDealValue t
WHERE LegalEntityName = 'Sequoia Portfolio' AND t.PeriodDate = a.PeriodDate) AS PortNAV
FROM dbo.udf_CombinedConsolidatedReturns(96, NULL, 'Real Estate & Infrastructure', 'Direct', NULL, '01/01/2011', '11/30/2016') a
UNION ALL
SELECT LegalEntityName, AssetClass AS 'SAC', 'Policy' AS 'FundDirect', 'Policy' AS 'ActualPolicy', PeriodDate, BmReturn, 0 AS EndingValue, 0 AS PortNAV
FROM [CapricornUser].dbo.usr_LTPolicy
WHERE LegalEntityName = 'Sequoia Portfolio'
AND AssetClass IN ('Natural Resources', 'Real Estate & Infrastructure')
AND [Level] = 2
AND PeriodDate > '12/31/10'
")
str(qry)
qry$PeriodDate = as.Date(qry$PeriodDate)
qry$Year = as.numeric(format(qry$PeriodDate, '%Y'))
qry_summary = qry %>% group_by(LegalEntityName, SAC, FundDirect, Year) %>%
summarise(ReturnCum = exp(sum(log(1 + Return)))-1, AvgWeight = mean(EndingValue)/mean(PortNAV))
qry_combined = qry %>% group_by(LegalEntityName, SAC, FundDirect) %>%
summarise(ReturnCum = exp(sum(log(1 + Return)))^(12/n())-1, AvgWeight = mean(EndingValue)/mean(PortNAV)) %>%
mutate(Year = '2011-2016')
qry_summary$Year = as.character(qry_summary$Year)
qry_summary = bind_rows(qry_summary, qry_combined)
qry_summary$Year = factor(qry_summary$Year, levels = c('2011', '2012', '2013', '2014', '2015', '2016', '2011-2016'))
p1 = qry_summary %>%
ggplot(aes(x = Year, y = ReturnCum, fill = FundDirect)) +
geom_bar(stat = 'identity', position = position_dodge()) +
geom_text(aes(label = format(ReturnCum*100, digits = 0)), size = 3,
position = position_dodge(width = 1), vjust = -0.25) +
facet_grid(. ~ SAC) +
scale_y_continuous(labels = percent) +
scale_fill_economist() +
theme(axis.title.x=element_blank()) +
ylab('TWR') +
ggtitle('Private Fund & Direct Returns')
p2 = qry_summary %>%
ggplot(aes(x = Year, y = AvgWeight, fill = FundDirect)) +
geom_bar(stat = 'identity', position = position_dodge()) +
geom_text(aes(label = format(AvgWeight*100, digits = 0)), size = 3,
position = position_dodge(width = 1), vjust = -0.25) +
facet_grid(. ~ SAC) +
scale_y_continuous(labels = percent) +
scale_fill_economist() +
theme(axis.title.x=element_blank()) +
ylab('Avg NAV')
grid.arrange(p1, p2, nrow = 2, heights = c(2, 1))
source('G:/INVESTMENT TEAM/Invest Statements/Internal Reporting/R/ReturnAnalysis_FundDirectSAC.R', echo=TRUE)
setwd("C:/Users/bhao/Google Drive/CUNY/git/DATA605/HW6")
library(RTextTools)
texts = paste(readLines(paste0(getwd(), '/assign6.sample.txt')), collapse = ' ')
str(texts)
# create
matrix = create_matrix(textColumns = texts, language = 'english', removeNumbers = TRUE, toLower = TRUE, removePunctuation = TRUE)
df = data.frame(term = matrix$dimnames$Terms, count = matrix$v)
length(matrix$v)
length(matrix$dimnames$Terms)
summary(matrix)
table(colSum(matrix))
table(colSums(matrix))
inspect(matrix)
library(tm)
inspect(matrix)
table(colSums(inspect(matrix)))
table(colSums(inspect(matrix)))
df = data.frame(term = matrix$dimnames$Terms, count = matrix$v)
df
library(dplyr)
df %>% summarise(n = n())
df %>% summarise(n = n()) %>%
mutate(pct = count / n)
df %>% group_by(term) %>%
summarise(n = n()) %>%
mutate(pct = count / sum(n))
df %>% group_by(term) %>%
summarise(n = n())
df %>% group_by(term) %>%
summarise(n = n()) %>%
mutate(pct = n / sum(n))
df %>% group_by(term) %>%
mutate(pct = count / sum(count))
df %>%
mutate(pct = count / sum(count))
df %>% mutate(sum(count))
df %>% summarise(sum(count))
df %>%  mutate(pct = count / sum(count))
df %>% mutate(pct = count / sum(count)) %>%
arrange(desc(pct))
matrix$j
df = data.frame(term = matrix$dimnames$Terms[matrix$j], count = matrix$v)
df %>% mutate(pct = count / sum(count)) %>%
arrange(desc(pct))
df %>% filter(term == 'the')
matrix = create_matrix(textColumns = texts, language = 'english', removeNumbers = TRUE, toLower = TRUE,
removePunctuation = TRUE, stemWords = FALSE, stopwords = TRUE)
?create_matrix
matrix = create_matrix(textColumns = texts, language = 'english', removeNumbers = TRUE, toLower = TRUE,
removePunctuation = TRUE, stemWords = FALSE, removeStopwords = FALSE)
df = data.frame(term = matrix$dimnames$Terms[matrix$j], count = matrix$v)
df %>% mutate(pct = count / sum(count)) %>%
arrange(desc(pct))
library(RTextTools)
library(tm)
texts = paste(readLines(paste0(getwd(), '/assign6.sample.txt')), collapse = ' ')
str(texts)
# create document-term matrix with i columns for each document (1 in this case),
# j rows for each uniue term, and matrix v for count of each term within each document
matrix1 = create_matrix(textColumns = texts, language = 'english', removeNumbers = FALSE, removePunctuation = TRUE,
removeStopwords = FALSE,  stemWords = FALSE, stripWhitespace = TRUE, toLower = TRUE,
ngramLength = 1)
# create data frame of terms and term counts
df = data.frame(term = matrix1$dimnames$Terms[matrix1$j], count = matrix1$v)
df %>% mutate(pct = count / sum(count)) %>%
arrange(desc(pct))
matrix2 = create_matrix(textColumns = texts, language = 'english', removeNumbers = FALSE, removePunctuation = TRUE,
removeStopwords = FALSE,  stemWords = FALSE, stripWhitespace = TRUE, toLower = TRUE,
ngramLength = 2)
df = data.frame(term = matrix2$dimnames$Terms[matrix2$j], count = matrix2$v)
df %>% mutate(pct = count / sum(count)) %>%
arrange(desc(pct))
matrix2
matrix1
trace(create_matrix,edit=T)
matrix2 = create_matrix(textColumns = texts, language = 'english', removeNumbers = FALSE, removePunctuation = TRUE,
removeStopwords = FALSE,  stemWords = FALSE, stripWhitespace = TRUE, toLower = TRUE,
ngramLength = 2)
df = data.frame(term = matrix2$dimnames$Terms[matrix2$j], count = matrix2$v)
df %>% mutate(pct = count / sum(count)) %>%
arrange(desc(pct))
trace(create_matrix,edit=T)
matrix2 = create_matrix(textColumns = texts, language = 'english', removeNumbers = FALSE, removePunctuation = TRUE,
removeStopwords = FALSE,  stemWords = FALSE, stripWhitespace = TRUE, toLower = TRUE,
ngramLength = 3)
matrix2
df = data.frame(term = matrix2$dimnames$Terms[matrix2$j], count = matrix2$v)
df %>% mutate(pct = count / sum(count)) %>%
arrange(desc(pct))
install.packages("RWeka")
library(RWeka)
bigramTokenizer = function(x) {
NGramTokenizer(x, Weka_control(min = 2, max = 2))
}
tdm_bigram = TermDocumentMatrix(texts, control = list(tokenize = bigramTokenizer))
text_corpus = Corpus(texts)
text_corpus = Corpus(VectorSource(texts))
tdm_bigram = TermDocumentMatrix(texts, control = list(tokenize = bigramTokenizer))
tdm_bigram = TermDocumentMatrix(text_corpus, control = list(tokenize = bigramTokenizer))
tdm_bigram
df = data.frame(term = tdm_bigram$dimnames$Terms[tdm_bigram$j], count = tdm_bigram$v)
df %>% mutate(pct = count / sum(count)) %>%
arrange(desc(pct))
tdm_bigram
tdm_bigram$dimnames$Terms
tdm_bigram
bigramTokenizer = function(x) {
NGramTokenizer(x, Weka_control(min = 2, max = 2))
}
tdm_bigram = TermDocumentMatrix(text_corpus, control = list(tokenize = bigramTokenizer))
tdm_bigram$dimnames$Terms
length(tdm_bigram$dimnames$Terms)
inspect(tdm_bigram)
library(RTextTools)
library(tm)
texts = paste(readLines(paste0(getwd(), '/assign6.sample.txt')), collapse = ' ')
str(texts)
term_freq = function(texts) {
# create document-term matrix with i columns for each document (1 in this case),
# j rows for each uniue term, and matrix v for count of each term within each document
tdm = create_matrix(textColumns = texts, language = 'english', removeNumbers = FALSE, removePunctuation = TRUE,
removeStopwords = FALSE,  stemWords = FALSE, stripWhitespace = TRUE, toLower = TRUE)
# create data frame of terms and term counts
df = data.frame(term = tdm$dimnames$Terms[tdm$j], count = tdm$v) %>%
mutate(pct = count / sum(count)) %>%
arrange(desc(pct))
}
matrix2 = create_matrix(textColumns = texts, language = 'english', removeNumbers = FALSE, removePunctuation = TRUE,
removeStopwords = FALSE,  stemWords = FALSE, stripWhitespace = TRUE, toLower = TRUE,
ngramLength = 3)
term_freq(texts)
term_freq = function(texts) {
# create document-term matrix with i columns for each document (1 in this case),
# j rows for each uniue term, and matrix v for count of each term within each document
tdm = create_matrix(textColumns = texts, language = 'english', removeNumbers = FALSE, removePunctuation = TRUE,
removeStopwords = FALSE,  stemWords = FALSE, stripWhitespace = TRUE, toLower = TRUE)
# create data frame of terms and term counts
data.frame(term = tdm$dimnames$Terms[tdm$j], count = tdm$v) %>%
mutate(pct = count / sum(count)) %>%
arrange(desc(pct))
}
term_freq(texts)
term_freq = function(texts) {
# create document-term matrix with i columns for each document (1 in this case),
# j rows for each uniue term, and matrix v for count of each term within each document
tdm = create_matrix(textColumns = texts, language = 'english', removeNumbers = FALSE, removePunctuation = TRUE,
removeStopwords = FALSE,  stemWords = FALSE, stripWhitespace = TRUE, toLower = TRUE)
# create data frame of terms and term counts
data.frame(term = tdm$dimnames$Terms[tdm$j], count = tdm$v) %>%
mutate(freq = count / sum(count)) %>%
arrange(desc(freq))
}
cleanse_corpus = function(x) {
x = tm_map(x, PlainTextDocument)  # to make tm package play nice
x = tm_map(x, removeNumbers)
x = tm_map(x, removePunctuation)  # leaving punctuation in case relevant to spam ham filter
x = tm_map(x, stripWhitespace)
# x = tm_map(x, stemDocument)
x = tm_map(x, content_transformer(tolower))  # content_transformer required because tolower not tm function
# x = tm_map(x, removeWords, stopwords('en'))
}
text_corpus = Corpus(VectorSource(texts))
text_corpus = cleanse_corpus(text_corpus)
text_corpus
text_corpus$`1`
text_corpus
text_corpus$1
text_corpus[[1]]
text_corpus[[1]]$content
library(stringr)
str_count(text_corpus[[1]]$content, 'the')
str_count(text_corpus[[1]]$content, "\bthe\b"")
}
str_count(text_corpus[[1]]$content, "\bthe\b")
str_count(text_corpus[[1]]$content, "\bthe\b")
str_count(text_corpus[[1]]$content, "\\bthe\\b")
term_freq(texts)
str_count(text_corpus[[1]]$content, "\\band\\b")
word1 = 'and'
str_count(text_corpus[[1]]$content, paste0("\\b", word1, "\\b")
str_count(text_corpus[[1]]$content, paste0("\\b", word1, "\\b"))
paste0("\\b", word1, "\\b")
str_count(text_corpus[[1]]$content, paste0("\\b", word1, "\\b"))
texts = text_corpus[[1]]$content
length(texts)
texts
str(texts)
text_corpus
text_corpus[[1]]$meta
text_corpus = Corpus(VectorSource(texts))
text_corpus = cleanse_corpus(text_corpus)
dtm = DocumentTermMatrix(text_corpus)
dtm
df = data.frame(term = dtm$dimnames$Terms[dtm$j], count = dtm$v)
word2 = 'the'
df = data.frame(term = dtm$dimnames$Terms[dtm$j], count = dtm$v) %>%
mutate(freq = count / sum(count)) %>%
filter(term == word1 | term == word2)
df
paste0("\\b", word1, "\\b\s\\b", word2, "\\b")
paste0("\\b", word1, "\\b\\s\\b", word2, "\\b")
str_count(texts, paste0("\\b", word1, "\\b\\s\\b", word2, "\\b"))
dtm
sum(dtm$v)
data.frame(term = dtm$dimnames$Terms[dtm$j], count = dtm$v) %>% summarise(sum(count))
bigram_freq = function(texts, word1, word2) {
# convert texts to corpus
text_corpus = Corpus(VectorSource(texts))
text_corpus = cleanse_corpus(text_corpus)
dtm = DocumentTermMatrix(text_corpus)
# data frame to house individual probabilities
df = data.frame(term = dtm$dimnames$Terms[dtm$j], count = dtm$v) %>%
mutate(freq = count / sum(count)) %>%
filter(term == word1 | term == word2)
# calculate joint probability of adjacent occurence
joint_count = str_count(texts, paste0("\\b", word1, "\\b\\s\\b", word2, "\\b")) +
str_count(texts, paste0("\\b", word2, "\\b\\s\\b", word1, "\\b"))
df %>% union(data.frame(term = 'bigram', count = joint_count, freq = joint_count / sum(dtm$v)))  # divide by total word count
}
bigram_freq(texts, 'and', 'the')
str(df)
df = data.frame(term = dtm$dimnames$Terms[dtm$j], count = dtm$v, stringsAsFactors = FALSE) %>%
mutate(freq = count / sum(count)) %>%
filter(term == word1 | term == word2)
str(df)
df %>% union(data.frame(term = 'bigram', count = joint_count, freq = joint_count / sum(dtm$v)))  # divide by total word count
joint_count = str_count(texts, paste0("\\b", word1, "\\b\\s\\b", word2, "\\b")) +
str_count(texts, paste0("\\b", word2, "\\b\\s\\b", word1, "\\b"))
df %>% union(data.frame(term = 'bigram', count = joint_count, freq = joint_count / sum(dtm$v)))  # divide by total word count
bigram_freq = function(texts, word1, word2) {
# convert texts to corpus
text_corpus = Corpus(VectorSource(texts))
text_corpus = cleanse_corpus(text_corpus)
dtm = DocumentTermMatrix(text_corpus)
# data frame to house individual probabilities
df = data.frame(term = dtm$dimnames$Terms[dtm$j], count = dtm$v, stringsAsFactors = FALSE) %>%
mutate(freq = count / sum(count)) %>%
filter(term == word1 | term == word2)
str(df)
# calculate joint probability of adjacent occurence
joint_count = str_count(texts, paste0("\\b", word1, "\\b\\s\\b", word2, "\\b")) +
str_count(texts, paste0("\\b", word2, "\\b\\s\\b", word1, "\\b"))
df %>% union(data.frame(term = 'bigram', count = joint_count, freq = joint_count / sum(dtm$v)))  # divide by total word count
}
bigram_freq(texts, 'and', 'the')
6^3
6^2
2/6^2
factorial(6)
factorial(365) / factorial(365-25) / 365^25
factorial(365)
364/365 * 363/365
364/365 * 363/365 * 362/365
same_bday_prob = function(n) {
prob = 1
for (i in 1:n) {
prob = prob * (365 - i) / 365
}
return(prob)
}
same_bday_prob(25)
same_bday_prob(50)
same_bday_prob = function(n) {
prob = 1
for (i in 1:n) {
prob = prob * (365 - i) / 365
}
return(1 - prob)
}
same_bday_prob(25)
same_bday_prob(50)
same_bday_prob(75)
same_bday_prob(23)
same_bday_prob = function(n) {
prob = 1
for (i in 1:(n-1)) {
prob = prob * (365 - i) / 365
}
return(1 - prob)
}
same_bday_prob(2)
same_bday_prob(23)
same_bday_prob(22)
same_bday_prob(23)
choose(5, 2)
choose(5, 3)
1 - (364/365)^choose(23, 2)
same_bday_prob(23)
same_bday_prob(23)
1 - (364/365)^choose(23, 2)
choose(365, 23) / factorial(23)
choose(365, 23) / factorial(23) / 365^23
choose(365, 23) / factorial(23) / 365^23
choose(365, 23) * factorial(23) / 365^23
1- choose(365, 23) * factorial(23) / 365^23
same_bday_prob2 = function(k) {
# we want 1 - (365 - k)! / 365^k
# since combination = n! / k!(n - k)!, just multiply by k!
return(1 - choose(365, k) * factorial(k) / 365^k)
}
same_bday_prob2(23)
same_bday_prob(23)
same_bday_prob2(23)
same_bday_prob2 = function(k) {
# we want 1 - 364/365 * 363/365 * ... * (365-k)/365, or
# 1 - [ 365! / (365 - k)! ] / 365^k
# since combination = n! / k!(n - k)!, just multiply by k!
return(1 - choose(365, k) * factorial(k) / 365^k)
}
same_bday_prob = function(k) {
# we want 1 - 364/365 * 363/365 * ... * (365-k)/365, or
# 1 - [ 365! / (365 - k)! ] / 365^k
# since combination = n! / k!(n - k)!, just multiply by k!
return(1 - choose(365, k) * factorial(k) / 365^k)
}
same_bday_prob(23)
# 1.
6^3
# 2. Combinations include 1 + 2 and 2 + 1 out of 6^2 total combinations
2/6^2
# 3.
same_bday_prob = function(k) {
# we want 1 - 364/365 * 363/365 * ... * (365-k)/365, or
# 1 - [ 365! / (365 - k)! ] / 365^k
# since combination = n! / k!(n - k)!, just multiply by k!
return(1 - choose(365, k) * factorial(k) / 365^k)
}
same_bday_prob(23)
same_bday_prob(50)
term_freq = function(texts) {
# create document-term matrix with i columns for each document (1 in this case),
# j rows for each uniue term, and matrix v for count of each term within each document
tdm = create_matrix(textColumns = texts, language = 'english', removeNumbers = FALSE, removePunctuation = TRUE,
removeStopwords = FALSE,  stemWords = FALSE, stripWhitespace = TRUE, toLower = TRUE)
# create data frame of terms and term counts
data.frame(term = tdm$dimnames$Terms[tdm$j], count = tdm$v) %>%
mutate(freq = round(count / sum(count), 4) %>%
arrange(desc(freq))
}
term_freq = function(texts) {
# create document-term matrix with i columns for each document (1 in this case),
# j rows for each uniue term, and matrix v for count of each term within each document
tdm = create_matrix(textColumns = texts, language = 'english', removeNumbers = FALSE, removePunctuation = TRUE,
removeStopwords = FALSE,  stemWords = FALSE, stripWhitespace = TRUE, toLower = TRUE)
# create data frame of terms and term counts
data.frame(term = tdm$dimnames$Terms[tdm$j], count = tdm$v) %>%
mutate(freq = round(count / sum(count), 4)) %>%
arrange(desc(freq))
}
head(term_freq(texts))
setwd("C:/Users/bhao/Google Drive/CUNY/git/DATA605/HW6")
library(dplyr)
library(RTextTools)
library(tm)
texts = paste(readLines(paste0(getwd(), '/assign6.sample.txt')), collapse = ' ')
term_freq = function(texts) {
# create document-term matrix with i columns for each document (1 in this case),
# j rows for each uniue term, and matrix v for count of each term within each document
tdm = create_matrix(textColumns = texts, language = 'english', removeNumbers = FALSE, removePunctuation = TRUE,
removeStopwords = FALSE,  stemWords = FALSE, stripWhitespace = TRUE, toLower = TRUE)
# create data frame of terms and term counts
data.frame(term = tdm$dimnames$Terms[tdm$j], count = tdm$v) %>%
mutate(freq = round(count / sum(count), 4)) %>%
arrange(desc(freq))
}
head(term_freq(texts))
library(stringr)
texts = paste(readLines(paste0(getwd(), '/assign6.sample.txt')), collapse = ' ')
# function to cleanse corpus
cleanse_corpus = function(x) {
x = tm_map(x, PlainTextDocument)  # to make tm package play nice
x = tm_map(x, removeNumbers)
x = tm_map(x, removePunctuation)  # leaving punctuation in case relevant to spam ham filter
x = tm_map(x, stripWhitespace)
# x = tm_map(x, stemDocument)
x = tm_map(x, content_transformer(tolower))  # content_transformer required because tolower not tm function
# x = tm_map(x, removeWords, stopwords('en'))
}
bigram_freq = function(texts, word1, word2) {
# convert texts to corpus
text_corpus = Corpus(VectorSource(texts))
text_corpus = cleanse_corpus(text_corpus)
dtm = DocumentTermMatrix(text_corpus)
# data frame to house individual probabilities
df = data.frame(term = dtm$dimnames$Terms[dtm$j], count = dtm$v, stringsAsFactors = FALSE) %>%
mutate(freq = round(count / sum(count), 4)) %>%
filter(term == word1 | term == word2)
# calculate joint probability of adjacent occurence
joint_count = str_count(texts, paste0("\\b", word1, "\\b\\s\\b", word2, "\\b")) +
str_count(texts, paste0("\\b", word2, "\\b\\s\\b", word1, "\\b"))
df %>% union(data.frame(term = 'bigram', count = joint_count, freq = round(joint_count / sum(dtm$v), 4)))  # divide by total word count
}
bigram_freq(texts, 'and', 'the')
term_freq(texts)
term_freq(texts) %>% filter(term == word1 | term == word2)
bigram_freq = function(texts, word1, word2) {
# convert texts to corpus
text_corpus = Corpus(VectorSource(texts))
text_corpus = cleanse_corpus(text_corpus)
dtm = DocumentTermMatrix(text_corpus)
# data frame to house individual probabilities
# df = data.frame(term = dtm$dimnames$Terms[dtm$j], count = dtm$v, stringsAsFactors = FALSE) %>%
#   mutate(freq = round(count / sum(count), 4)) %>%
#   filter(term == word1 | term == word2)
df = term_freq(texts) %>% filter(term == word1 | term == word2)
# calculate joint probability of adjacent occurence
joint_count = str_count(texts, paste0("\\b", word1, "\\b\\s\\b", word2, "\\b")) +
str_count(texts, paste0("\\b", word2, "\\b\\s\\b", word1, "\\b"))
df %>% union(data.frame(term = 'bigram', count = joint_count, freq = round(joint_count / sum(dtm$v), 4)))  # divide by total word count
}
bigram_freq(texts, 'and', 'the')
term_freq = function(texts) {
# create document-term matrix with i columns for each document (1 in this case),
# j rows for each uniue term, and matrix v for count of each term within each document
tdm = create_matrix(textColumns = texts, language = 'english', removeNumbers = FALSE, removePunctuation = TRUE,
removeStopwords = FALSE,  stemWords = FALSE, stripWhitespace = TRUE, toLower = TRUE)
# create data frame of terms and term counts
data.frame(term = tdm$dimnames$Terms[tdm$j], count = tdm$v, stringsAsFactors = FALSE) %>%
mutate(freq = round(count / sum(count), 4)) %>%
arrange(desc(freq))
}
bigram_freq = function(texts, word1, word2) {
# convert texts to corpus
text_corpus = Corpus(VectorSource(texts))
text_corpus = cleanse_corpus(text_corpus)
dtm = DocumentTermMatrix(text_corpus)
# data frame to house individual probabilities
# df = data.frame(term = dtm$dimnames$Terms[dtm$j], count = dtm$v, stringsAsFactors = FALSE) %>%
#   mutate(freq = round(count / sum(count), 4)) %>%
#   filter(term == word1 | term == word2)
df = term_freq(texts) %>% filter(term == word1 | term == word2)
# calculate joint probability of adjacent occurence
joint_count = str_count(texts, paste0("\\b", word1, "\\b\\s\\b", word2, "\\b")) +
str_count(texts, paste0("\\b", word2, "\\b\\s\\b", word1, "\\b"))
df %>% union(data.frame(term = 'bigram', count = joint_count, freq = round(joint_count / sum(dtm$v), 4)))  # divide by total word count
}
bigram_freq(texts, 'and', 'the')
