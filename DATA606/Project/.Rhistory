publisher = data_xml %>% xml_find_all('//publisher') %>% xml_text()
books_xml = data.frame(bookName, authorName1)
books_xml
books_xml = data.frame(bookName, authorName1, authorName2, genre, publisher)
books_xml
authorName2 = data_xml %>% xml_find_all('//authorName2') %>% xml_text()
authorName2
authorName1 = data_xml %>% xml_find_all('//authorName1') %>% xml_text()
authorName1
url_xml = getURL('https://raw.githubusercontent.com/haobruce/CUNY/master/DATA607/Week7/Hao-Week7_XML.xml')
# using xml2 is more intuitive but this method does not handle the second author case
data_xml = read_xml(url_xml) %>% xml_find_all('//book')
bookName = data_xml %>% xml_find_all('//bookName') %>% xml_text()
authorName1 = data_xml %>% xml_find_all('//authorName1') %>% xml_text()
authorName2 = data_xml %>% xml_find_all('//authorName2') %>% xml_text()
genre = data_xml %>% xml_find_all('//genre') %>% xml_text()
publisher = data_xml %>% xml_find_all('//publisher') %>% xml_text()
books_xml = data.frame(bookName, authorName1, authorName2, genre, publisher)
books_xml
books_xml
print(books_xml)
books_xml
library(RCurl)
library(XML)
library(xml2)
library(rjson)
library(dplyr)
setwd("C:/Users/bhao/Google Drive/CUNY/git/DATA607/Week7")
url_html = getURL('https://raw.githubusercontent.com/haobruce/CUNY/master/DATA607/Week7/Hao-Week7_HTML.html')
books_html = readHTMLTable(url_html, stringsAsFactors = F)
books_html
data = xmlParse(url_xml)
data
bookName = as.list(data[['books']][['book']][['bookName']])
as.list(data[['books']][['book']][['bookName']])
as.list(data[['book']][['bookName']])
data = xmlToList(data)
bookName = as.list(data[['books']][['book']][['bookName']])
bookName
bookName = data[['books']][['book']][['bookName']]
bookName
bookName = data[['books']][['book']]
bookName
xmlToDataFrame(url_xml)
install.packages('XPath')
doc = xmlTreeParse(url_xml)
doc
top = xmlRoot(doc)
top
xmlName(top)
url_xml = getURL('https://raw.githubusercontent.com/haobruce/CUNY/master/DATA607/Week7/Hao-Week7_XML.xml')
# using xml2 is more intuitive but this method does not handle the second author case
data_xml = read_xml(url_xml) %>% xml_find_all('//book')
bookName = data_xml %>% xml_find_all('//bookName') %>% xml_text()
authorName1 = data_xml %>% xml_find_all('//authorName1') %>% xml_text()
authorName2 = data_xml %>% xml_find_all('//authorName2') %>% xml_text()
genre = data_xml %>% xml_find_all('//genre') %>% xml_text()
publisher = data_xml %>% xml_find_all('//publisher') %>% xml_text()
books_xml = data.frame(bookName, authorName1, authorName2, genre, publisher)
books_xml
books_xml
xmlToDataFrame(url_xml)
xmlParse(url_xml2)
url_xml2 = getURL('https://raw.githubusercontent.com/haobruce/CUNY/master/DATA607/Week7/Hao-Week7_XML2.xml')
xmlParse(url_xml2)
xmlParse(url_xml2)[[1]]
xmlToList(xmlParse(url_xml2))
url_xml = getURL('https://raw.githubusercontent.com/haobruce/CUNY/master/DATA607/Week7/Hao-Week7_XML.xml')
url_xml2 = getURL('https://raw.githubusercontent.com/haobruce/CUNY/master/DATA607/Week7/Hao-Week7_XML2.xml')
# try using XML again
data_xml = xmlParse(url_xml)
data_xml = xmlToList(data_xml)
bookName = data_xml[['books']][['book']][['bookName']]
bookName
data_xml
url_xml
xmlParse(url_xml)
xmlToDataFrame(xmlParse(url_xml))
url_xml = getURL('https://raw.githubusercontent.com/haobruce/CUNY/master/DATA607/Week7/Hao-Week7_XML.xml')
url_xml2 = getURL('https://raw.githubusercontent.com/haobruce/CUNY/master/DATA607/Week7/Hao-Week7_XML2.xml')
data_xml = xmlParse(url_xml)
bookName = xmlToDataFrame(data_xml)
bookName
bookName
print(bookName)
str(bookName)
str(books_html)
url_xml = getURL('https://raw.githubusercontent.com/haobruce/CUNY/master/DATA607/Week7/Hao-Week7_XML.xml')
url_xml2 = getURL('https://raw.githubusercontent.com/haobruce/CUNY/master/DATA607/Week7/Hao-Week7_XML2.xml')
data_xml = xmlParse(url_xml)
books_xml = xmlToDataFrame(data_xml)
books_xml
books_xml
books_html
books_html[[1]]
url_html = getURL('https://raw.githubusercontent.com/haobruce/CUNY/master/DATA607/Week7/Hao-Week7_HTML.html')
books_html = readHTMLTable(url_html, stringsAsFactors = F)[[1]]
books_html
url_xml2 = getURL('https://raw.githubusercontent.com/haobruce/CUNY/master/DATA607/Week7/Hao-Week7_XML2.xml')
library(RCurl)
library(XML)
library(xml2)
library(rjson)
library(dplyr)
setwd("C:/Users/bhao/Google Drive/CUNY/git/DATA607/Week7")
url_xml2 = getURL('https://raw.githubusercontent.com/haobruce/CUNY/master/DATA607/Week7/Hao-Week7_XML2.xml')
xmlToDataFrame(xmlParse(data_xml))
xmlToDataFrame(xmlParse(url_xml2))
xmlParse(url_xml2)
xmlParse(url_xml2)[[1]]
?xmlParse
xmlParse(url_xml2, parentFirst = T)
xmlInternalTreeParse(url_xml2)
xmlParse(url_xml2)
xmlParse(xmlParse(url_xml2))
xmlRoot(url_xml2)
xmlRoot(xmlParse(url_xml2))
xmlRoot(xmlRoot(xmlParse(url_xml2)))
xmlRoot(xmlParse(url_xml2))[[1]]
url_xml2 = getURL('https://raw.githubusercontent.com/haobruce/CUNY/master/DATA607/Week7/Hao-Week7_XML2.xml')
data_xml2 = xmlParse(url_xml2)
data_xml2 = xmlRoot(data_xml2)
books_xml2 = xmlToDataFrame(data_xml2)
books_xml2
books_xml2
data_xml2
data_xml2 = xmlParse(url_xml2)
data_xml2 = xmlRoot(data_xml2)[[1]]
books_xml2 = xmlToDataFrame(data_xml2)
books_xml2
url_xml = getURL('https://raw.githubusercontent.com/haobruce/CUNY/master/DATA607/Week7/Hao-Week7_XML.xml')
books_xml = xmlToDataFrame(data_xml)
books_xml
url_xml = getURL('https://raw.githubusercontent.com/haobruce/CUNY/master/DATA607/Week7/Hao-Week7_XML.xml')
url_xml = getURL('https://raw.githubusercontent.com/haobruce/CUNY/master/DATA607/Week7/Hao-Week7_XML.xml')
data_xml = xmlParse(url_xml)
books_xml = xmlToDataFrame(data_xml)
books_xml
url_xml = getURL('https://raw.githubusercontent.com/haobruce/CUNY/master/DATA607/Week7/Hao-Week7_XML.xml')
data_xml = xmlParse(url_xml)
books_xml = xmlToDataFrame(data_xml)
books_xml
# case with extra parent node
url_xml2 = getURL('https://raw.githubusercontent.com/haobruce/CUNY/master/DATA607/Week7/Hao-Week7_XML2.xml')
data_xml2 = xmlParse(url_xml2)
data_xml2 = xmlRoot(data_xml2)[[1]]
books_xml2 = xmlToDataFrame(data_xml2)
books_xml2
url_json = getURL('https://raw.githubusercontent.com/haobruce/CUNY/master/DATA607/Week7/Hao-Week7_JSON.json')
rjson(url_json)
library(RCurl)
library(XML)
library(rjson)
library(dplyr)
setwd("C:/Users/bhao/Google Drive/CUNY/git/DATA607/Week7")
library(dplyr)
rjson(url_json)
rjson::fromJSON(url_json)
fromJSON(url_json)
str(fromJSON(url_json))
install.packages('jsonlite')
library(jsonlite)
fromJSON(url_json)
fromJSON(url_json)
fromJSON(url_json)[[1]]
fromJSON(url_json)[[1]]
knitr::kable(books_xml2)
library(DT)
datatable(books_html)
url_html = getURL('https://raw.githubusercontent.com/haobruce/CUNY/master/DATA607/Week7/Hao-Week7_HTML.html')
books_html = readHTMLTable(url_html, stringsAsFactors = F)[[1]]
datatable(books_html)
glimpse(books_xml)
tbl_df(books_xml)
url_xml = getURL('https://raw.githubusercontent.com/haobruce/CUNY/master/DATA607/Week7/Hao-Week7_XML.xml')
data_xml = xmlParse(url_xml)
books_xml = xmlToDataFrame(data_xml)
tbl_df(books_xml)
# case with extra parent node
url_xml2 = getURL('https://raw.githubusercontent.com/haobruce/CUNY/master/DATA607/Week7/Hao-Week7_XML2.xml')
data_xml2 = xmlParse(url_xml2)
data_xml2 = xmlRoot(data_xml2)[[1]]
books_xml2 = xmlToDataFrame(data_xml2)
knitr::kable(books_xml2)
?kable
url_xml = getURL('https://raw.githubusercontent.com/haobruce/CUNY/master/DATA607/Week7/Hao-Week7_XML.xml')
data_xml = xmlParse(url_xml)
books_xml = xmlToDataFrame(data_xml)
tbl_df(books_xml)
# case with extra parent node
url_xml2 = getURL('https://raw.githubusercontent.com/haobruce/CUNY/master/DATA607/Week7/Hao-Week7_XML2.xml')
data_xml2 = xmlParse(url_xml2)
data_xml2 = xmlRoot(data_xml2)[[1]]
books_xml2 = xmlToDataFrame(data_xml2)
knitr::kable(books_xml2)
url_json = getURL('https://raw.githubusercontent.com/haobruce/CUNY/master/DATA607/Week7/Hao-Week7_JSON.json')
books_json = fromJSON(url_json)[[1]]
books_json
library(RCurl)
library(XML)
library(jsonlite)
library(dplyr)
library(DT)
setwd("C:/Users/bhao/Google Drive/CUNY/git/DATA607/Week7")
options(width=100)
url_html = getURL('https://raw.githubusercontent.com/haobruce/CUNY/master/DATA607/Week7/Hao-Week7_HTML.html')
books_html = readHTMLTable(url_html, stringsAsFactors = F)[[1]]
books_html
url_xml = getURL('https://raw.githubusercontent.com/haobruce/CUNY/master/DATA607/Week7/Hao-Week7_XML.xml')
data_xml = xmlParse(url_xml)
books_xml = xmlToDataFrame(data_xml)
books_xml
# case with extra parent node
url_xml2 = getURL('https://raw.githubusercontent.com/haobruce/CUNY/master/DATA607/Week7/Hao-Week7_XML2.xml')
data_xml2 = xmlParse(url_xml2)
data_xml2 = xmlRoot(data_xml2)[[1]]
books_xml2 = xmlToDataFrame(data_xml2)
books_xml2
books_html = readHTMLTable(url_html, stringsAsFactors = F)
books_html
str(books_html)
books_json = fromJSON(url_json)
books_json
str(books_json)
plot(cars)
plot(cars)
library(RCurl)
library(XML)
install.packages('rjsonlite')
install.packages('rjsonlite')
library(jsonlite)
library(RCurl)
library(XML)
library(jsonlite)
url = getURL('http://www.r-datacollection.com/materials/ch-3-xml/bond.xml')
url = getURL('http://www.r-datacollection.com/materials/ch-3-xml/bond.xml')
url
xml = xmlToDataFrame(url())
data = xmlToDataFrame(url)
data
str(data)
data
library(RCurl)
library(XML)
library(jsonlite)
url1 = getURL('http://www.r-datacollection.com/materials/ch-3-xml/bond.xml')
data = xmlToDataFrame(url1)
data
url2 = getURL('http://www.r-datacollection.com/materials/ch-3-xml/indy.json')
url2
url2 = fromJSON(url2)
url2 = getURL('http://www.r-datacollection.com/materials/ch-3-xml/indy.json')
data = fromJSON(url2)
data
data$`indy movies`$actors
data1 = xmlToDataFrame(url1)
data1
toJSON(data1)
fromJSON(toJSON(data1))
data2 = fromJSON(url2)
data2
url3 = getURL('http://www.r-datacollection.com/materials/ch-3-xml/technology-short.xml')
data3 = xmlToDataFrame(url3)
data3
library("XML")
library("stringr")
library("ggplot2")
library("plyr")
library("data.table")
suffix <- "cbs"
#Download fantasy football projections from cbssports.com
cbs_baseurl <- "http://fantasynews.cbssports.com/fantasyfootball/stats/weeklyprojections/"
cbs_pos <- c("QB","RB","WR","TE","K","DST")
cbs_writers <- c("jamey_eisenberg","dave_richard")
cbs_source <- c("cbs1", "cbs2")
cbs_leaguetype <- "standard"
cbs_urls <- paste0(cbs_baseurl, cbs_pos, "/season/", rep(cbs_writers, each=length(cbs_pos)), "/", cbs_leaguetype, "?&print_rows=9999")
cbs <- lapply(cbs_urls, function(x) {data.table(readHTMLTable(x, stringsAsFactors = FALSE)[7]$'NULL')})
cbs_urls
cbs
cbsList <- cbs
cbs_urls[1]
readHTMLTable(cbs_urls[1], stringsAsFactors = F)
readHTMLTable(cbs_urls[1], stringsAsFactors = F)[7]
data.table(readHTMLTable(cbs_urls[1], stringsAsFactors = F))
dt = data.table(readHTMLTable(cbs_urls[1], stringsAsFactors = F))
dt$V1
dt[[1]]
dt[1]
cbs <- lapply(cbs_urls, function(x) {data.table(readHTMLTable(x, stringsAsFactors = FALSE)[[1]])})
cbs
str(cbs)
head(cbs)
cbs[[1]]
head(cbs[[1]])
cbs_url[1]
cbs_urls[1]
dt = data.table(readHTMLTable(cbs_urls[1], stringsAsFactors = F))
readHTMLList(cbs_urls[1])
readHTMLList(cbs_urls[1], stringsAsFactors = F)
data.table(readHTMLList(cbs_urls[1], stringsAsFactors = F))
dt
readHTMLTable(cbs_urls[1], stringsAsFactors = F)
readHTMLTable(cbs_urls[1], stringsAsFactors = F)
xmlToDataFrame('http://www.r-datacollection.com/materials/ch-3-xml/technology-short.xml')
library(RCurl)
library(XML)
library(jsonlite)
data1 = xmlToDataFrame('http://www.r-datacollection.com/materials/ch-3-xml/bond.xml')
data1
toJSON(data1)
data2 = fromJSON('http://www.r-datacollection.com/materials/ch-3-xml/indy.json')
data1 = xmlToDataFrame('http://www.r-datacollection.com/materials/ch-3-xml/bond.xml')
data1
toJSON(data1)
data2 = fromJSON('http://www.r-datacollection.com/materials/ch-3-xml/indy.json')
url2 = getURL('http://www.r-datacollection.com/materials/ch-3-xml/indy.json')
data2 = fromJSON(url2)
data2
parsed_doc = htmlParse('http://www.r-datacollection.com/materials/ch-4-xpath/fortunes/fortunes.html')
parsed_doc
xpathSApply(doc = parsed_doc, path = '/html/body/div/p/i')
xpathSApply(doc = parsed_doc, path = '//body//i')
xpathSApply(doc = parsed_doc, path = '/html/body/div/p/i')
xpathSApply(doc = parsed_doc, path = '//body//i')
xpathSApply(doc = parsed_doc, path = '//body//p/i')
parsed_doc = htmlParse('http://www.r-datacollection.com/materials/ch-4-xpath/fortunes/fortunes.html')
parsed_doc = htmlParse('http://www.r-datacollection.com/materials/ch-4-xpath/fortunes/fortunes.html')
library(RCurl)
library(XML)
library(jsonlite)
data1 = xmlToDataFrame('http://www.r-datacollection.com/materials/ch-3-xml/bond.xml')
data1
toJSON(data1)
url2 = getURL('http://www.r-datacollection.com/materials/ch-3-xml/indy.json')
data2 = fromJSON(url2)
data2
data2$`indy movies`$actors
data3 = xmlToDataFrame('http://www.r-datacollection.com/materials/ch-3-xml/technology-short.xml')
data3
parsed_doc = htmlParse('http://www.r-datacollection.com/materials/ch-4-xpath/fortunes/fortunes.html')
parsed_doc
xpathSApply(doc = parsed_doc, path = '/html/body/div/p/i')
xpathSApply(doc = parsed_doc, path = '//body//p/i')
xpathSApply(doc = parsed_doc, path = '//body//p/i')
xpathSApply(doc = parsed_doc, path = '/html/body/div/p/i')
parsed_doc
xpathSApply(doc = parsed_doc, path = '//a/ancestor::div')
xpathSApply(doc = parsed_doc, path = '//a/ancestor::div//i')
parsed_doc
parsed_doc
xpathSApply(parsed_doc, '//body//p[position()=1]')
parsed_doc
xpathSApply(parsed_doc, '/html/body/div/p/i')
parsed_doc
xpathSApply(parsed_doc, '//p[count(.//a)>0]')
xpathSApply(parsed_doc, '//div[count(./@*)>2]')
xpathSApply(parsed_doc, '//title', fun = xmlValue)
parsed_doc
xpathSApply(parsed_doc, '//title', fun = xmlValue)
xmlToDataFrame('http://www.r-datacollection.com/materials/ch-3-xml/technology-short.xml')
xmlParse('http://www.r-datacollection.com/materials/ch-3-xml/technology-short.xml')
tech = xmlParse('http://www.r-datacollection.com/materials/ch-3-xml/technology-short.xml')
parsed_stocks = xmlParse('http://www.r-datacollection.com/materials/ch-3-xml/technology-short.xml')
xmlToDataFrame('http://www.r-datacollection.com/materials/ch-4-xpath/technology/technology.xml')
parsed_stocks = xmlParse('http://www.r-datacollection.com/materials/ch-4-xpath/technology/technology.xml')
companies = c('Apple', 'IBM', 'Google')
expQuery = sprintf('//%s/close', companies)
expQuery
getClose = function(node) {
value = xmlValue(node)
company = xmlName(xmlParent(node))
mat = c(company = company, value = value)
}
stocks = as.data.frame(t(xpathSApply(parsed_stocks, expQuery, getClose)))
stocks
stocks$value = as.numeric(as.character(stocks$value))
head(stocks)
?cat
library(RCurl)
library(XML)
library(jsonlite)
data1 = xmlToDataFrame('http://www.r-datacollection.com/materials/ch-3-xml/bond.xml')
data1
toJSON(data1)
url2 = getURL('http://www.r-datacollection.com/materials/ch-3-xml/indy.json')
data2 = fromJSON(url2)
data2
data2$`indy movies`$actors
data3 = xmlToDataFrame('http://www.r-datacollection.com/materials/ch-3-xml/technology-short.xml')
data3
parsed_doc = htmlParse('http://www.r-datacollection.com/materials/ch-4-xpath/fortunes/fortunes.html')
parsed_doc
xpathSApply(parsed_doc, '/html/body/div/p/i')
xpathSApply(parsed_doc, '//body//p/i')
xpathSApply(parsed_doc, '//a/ancestor::div//i')
xpathSApply(parsed_doc, '//body//p[position()=1]')
xpathSApply(parsed_doc, '//p[count(.//a)>0]')
xpathSApply(parsed_doc, '//div[count(./@*)>2]')
xpathSApply(parsed_doc, '//title', fun = xmlValue)
xmlToDataFrame('http://www.r-datacollection.com/materials/ch-4-xpath/technology/technology.xml')
parsed_stocks = xmlParse('http://www.r-datacollection.com/materials/ch-4-xpath/technology/technology.xml')
companies = c('Apple', 'IBM', 'Google')  # vector of company names
expQuery = sprintf('//%s/close', companies)  # create nodes based on company names
getClose = function(node) {  # extractor function
value = xmlValue(node)
company = xmlName(xmlParent(node))
mat = c(company = company, value = value)
}
stocks = as.data.frame(t(xpathSApply(parsed_stocks, expQuery, getClose)))
stocks$value = as.numeric(as.character(stocks$value))
head(stocks)
getForm(url, name = 'Eddie', age = 32)
cat(getForm(url, name = 'Eddie', age = 32))
cat(getForm(url, name = 'Eddie', age = '32'))
url = 'http://www.r-datacollection.com/materials/http/GETexample.com'
cat(getForm(url, name = 'Eddie', age = '32'))
url = 'http://www.r-datacollection.com/materials/http/GETexample.html'
cat(getForm(url, name = 'Eddie', age = '32'))
cat(getForm(url, name = 'Eddie', age = 32))
url = 'http://www.r-datacollection.com/materials/http/GETexample.php'
cat(getForm(url, name = 'Eddie', age = 32))
library(dbConnect)
username = 'data607'
password = 'project3'
dbname = 'dataScienceSkills'
host = 'data607-project3.ce2dfe0qxt5q.us-west-2.rds.amazonaws.com'
myDb = dbConnect(MySQL(), user=username, password=password, dbname=dbname, host=host)
selectQry = dbSendQuery(myDb, "SELECT * FROM skill_names;")
results = fetch(selectQry, n = -1)
results
myDb = dbConnect(MySQL(), user=username, password=password, dbname=dbname, host=host)
selectQry = dbSendQuery(myDb, "SELECT * FROM skill_names;")
myDb = dbConnect(MySQL(), user=username, password=password, dbname=dbname, host=host)
selectQry = dbSendQuery(myDb, "SELECT * FROM skill_names;")
results = fetch(selectQry, n = -1)
results
library(dbConnect)
username = 'data607'
password = 'project3'
dbname = 'dataScienceSkills'
host = 'data607-project3.ce2dfe0qxt5q.us-west-2.rds.amazonaws.com'
myDb = dbConnect(MySQL(), user=username, password=password, dbname=dbname, host=host)
importSkillNames = function(df) {
df$skill_name = paste0("(NULL, '", df$skill_name, "')")
values = paste(unlist(df, use.names = F), collapse = ', ')
qryString = paste("INSERT INTO skill_names (skill_id, skill_name) VALUES ", values, ";")
insertQry = dbSendQuery(myDb, qryString)
fetch(insertQry, n = -1)
}
selectQry = dbSendQuery(myDb, "SELECT * FROM skill_names;")
results = fetch(selectQry, n = -1)
results
df = as.data.frame(skill_name = c('Pandas', 'Regex'))
df = data.frame(skill_name = c('Pandas', 'Regex'))
df
?as.data.frame
df2 = as.data.frame(c('Pandas', 'Regex'), col.names = 'skill_name')
df2
df2 = as.data.frame(c('Pandas', 'Regex'), colnames = 'skill_name')
df2
df2 = as.data.frame(c('Pandas', 'Regex'), col.names = c('skill_name'))
df2
df = data.frame(skill_name = c('Pandas', 'Regex'))
importSkillNames(df)
selectQry = dbSendQuery(myDb, "SELECT * FROM skill_names;")
results = fetch(selectQry, n = -1)
results
qrString = 'SELECT jobs.post_id, role_desc, city, state, keyword_desc
FROM jobs
INNER JOIN locations
ON jobs.loc_id = locations.loc_id
INNER JOIN roles
ON jobs.role_id = roles.role_id
INNER JOIN post_keys
ON jobs.post_id = post_keys.post_id
INNER JOIN  keywords
ON post_keys.key_id = keywords.key_id
;'
qryString = 'SELECT jobs.post_id, role_desc, city, state, keyword_desc
FROM jobs
INNER JOIN locations
ON jobs.loc_id = locations.loc_id
INNER JOIN roles
ON jobs.role_id = roles.role_id
INNER JOIN post_keys
ON jobs.post_id = post_keys.post_id
INNER JOIN  keywords
ON post_keys.key_id = keywords.key_id
;'
selectQry = dbSendQuery(myDb, qryString)
library(dbConnect)
username = 'data607'
password = 'project3'
dbname = 'ds_skills'
host = 'data607-project3.ce2dfe0qxt5q.us-west-2.rds.amazonaws.com'
myDb = dbConnect(MySQL(), user=username, password=password, dbname=dbname, host=host)
qryString = 'SELECT jobs.post_id, role_desc, city, state, keyword_desc
FROM jobs
INNER JOIN locations
ON jobs.loc_id = locations.loc_id
INNER JOIN roles
ON jobs.role_id = roles.role_id
INNER JOIN post_keys
ON jobs.post_id = post_keys.post_id
INNER JOIN  keywords
ON post_keys.key_id = keywords.key_id
;'
selectQry = dbSendQuery(myDb, qryString)
results = fetch(selectQry, n = -1)
results
setwd("~/Google Drive/CUNY/git/DATA606/Project")
