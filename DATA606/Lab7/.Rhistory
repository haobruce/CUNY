tree_analytics = create_analytics(container, tree_out)
summary(tree_analytics)
maxent_analytics = create_analytics(container, maxent_out)
summary(maxent_analytics)
svm_out %>% ggplot(aes(x = SVM_PROB, y = SVM_LABEL)) + geom_point()
library(caret)
library(caretEnsemble)
library(parallel)
library(doParallel)  # for OSX use library(doMC)
library(caTools)
# convert dtm to matrix
full_mat = as.matrix(dtm)
train_mat = full_mat[1:round(N*0.8,0),]
test_mat = full_mat[(round(N*0.8,0)+1):N,]
# caret twoClassSummary requires non-numeric classes
spam_labels = factor(spam_labels, levels = c(0, 1), labels = c('ham', 'spam'))
train_y = spam_labels[1:round(N*0.8,0)]
test_y = spam_labels[(round(N*0.8,0)+1):N]
use_cores = detectCores()-1
cl = makeCluster(use_cores)
registerDoParallel(cl)  # for OSX use registerDoMC(cl)
myControl = trainControl(method = 'cv', number = 5, summaryFunction = twoClassSummary, classProbs = T, verboseIter = T)
svm = train(x = train_mat, y = train_y, method = 'svmRadial', metric = 'ROC', family = 'binomial', trControl = myControl)
svm
plot(svm)
svm_pred = predict(svm, newdata = test_mat, type = 'raw')
colAUC(as.numeric(svm_pred), test_y, plotROC = T)
confusionMatrix(svm_pred, test_y)
myControl = trainControl(summaryFunction = twoClassSummary, classProbs = T, verboseIter = T)
glm = train(x = train_mat, y = train_y, method = 'glmnet', metric = 'ROC', family = 'binomial', trControl = myControl)
glm
plot(glm)
myControl = trainControl(method = 'cv', number = 5, summaryFunction = twoClassSummary, classProbs = T, verboseIter = T)
glm = train(x = train_mat, y = train_y, method = 'glmnet', metric = 'ROC', family = 'binomial', trControl = myControl)
glm
plot(glm)
glm_pred = predict(glm, newdata = test_mat, type = 'raw')
colAUC(as.numeric(glm_pred), test_y, plotROC = T)
confusionMatrix(glm_pred, test_y)
model_list = list(glm = glm, svm = svm)
resamps = resamples(model_list)
summary(resamps)
dotplot(resamps, metric = 'ROC')
dotplot(resamps, metric = 'ROC')
confusionMatrix(glm_pred, test_y)
confusionMatrix(svm_pred, test_y)
confusionMatrix(glm_pred, test_y)
# svm confusion table
table(actual = labels_out[,1], predicted = labels_out[,2])
# tree confusion table
table(actual = labels_out[,1], predicted = labels_out[,3])
# maxent confusion table
table(actual = labels_out[,1], predicted = labels_out[,4])
# create container
container = create_container(dtm, labels = spam_labels,
trainSize = 1:round(N*0.8,0), testSize = (round(N*0.8,0)+1):N,  # use 80% of data for train
virgin = FALSE)
# estimation procedure
svm_model = train_model(container, 'SVM')
#cross_validate(container, nfold = 5, algorithm = 'SVM')
tree_model = train_model(container, 'TREE')
#cross_validate(container, nfold = 5, algorithm = 'TREE')
maxent_model = train_model(container, 'MAXENT')
#cross_validate(container, nfold = 5, algorithm = 'MAXENT')
svm_out = classify_model(container, svm_model)
tree_out = classify_model(container, tree_model)
maxent_out = classify_model(container, maxent_model)
# evaluation
labels_out = data.frame(
correct_label = spam_labels[(round(N*0.8,0)+1):N],
svm = as.character(svm_out[,1]),
tree = as.character(tree_out[,1]),
maxent = as.character(maxent_out[,1]),
stringsAsFactors = FALSE
)
# svm confusion table
table(actual = labels_out[,1], predicted = labels_out[,2])
# tree confusion table
table(actual = labels_out[,1], predicted = labels_out[,3])
# maxent confusion table
table(actual = labels_out[,1], predicted = labels_out[,4])
# analytics
svm_analytics = create_analytics(container, svm_out)
# create container
container = create_container(dtm, labels = spam_labels,
trainSize = 1:round(N*0.8,0), testSize = (round(N*0.8,0)+1):N,  # use 80% of data for train
virgin = FALSE)
# estimation procedure
svm_model = train_model(container, 'SVM')
#cross_validate(container, nfold = 5, algorithm = 'SVM')
tree_model = train_model(container, 'TREE')
#cross_validate(container, nfold = 5, algorithm = 'TREE')
maxent_model = train_model(container, 'MAXENT')
#cross_validate(container, nfold = 5, algorithm = 'MAXENT')
svm_out = classify_model(container, svm_model)
tree_out = classify_model(container, tree_model)
maxent_out = classify_model(container, maxent_model)
# evaluation
labels_out = data.frame(
correct_label = spam_labels[(round(N*0.8,0)+1):N],
svm = as.character(svm_out[,1]),
tree = as.character(tree_out[,1]),
maxent = as.character(maxent_out[,1]),
stringsAsFactors = FALSE
)
# svm confusion table
table(actual = labels_out[,1], predicted = labels_out[,2])
# tree confusion table
table(actual = labels_out[,1], predicted = labels_out[,3])
# maxent confusion table
table(actual = labels_out[,1], predicted = labels_out[,4])
# analytics
svm_analytics = create_analytics(container, svm_out)
svm_analytics = create_analytics(container, svm_out)
tree_analytics = create_analytics(container, tree_out)
maxent_analytics = create_analytics(container, maxent_out)
summary(svm_analytics)
summary(tree_analytics)
summary(maxent_analytics)
library(tm)
library(stringr)
library(XML)
library(RTextTools)
library(dplyr)
library(ggplot2)
#setwd("~/Google Drive/CUNY/git/DATA607/Week10")
setwd("C:/Users/bhao/Google Drive/CUNY/git/DATA607/Week10")
set.seed(123)
# create function to cleanse emails by removing non ASCII characters
cleanse_emails = function(dat) {
dat = str_c(dat, collapse = " ")
dat2 <- unlist(strsplit(dat, split=" "))
dat3 <- grep("dat2", iconv(dat2, "latin1", "ASCII", sub="dat2"))
if (length(dat3) == 0) {
dat4 = dat2
} else dat4 = dat2[-dat3]
dat5 <- paste(dat4, collapse = " ")
}
cleanse_corpus = function(x) {
x = tm_map(x, PlainTextDocument)  # to make tm package play nice
x = tm_map(x, removeNumbers)
x = tm_map(x, removePunctuation)  # leaving punctuation in case relevant to spam ham filter
x = tm_map(x, stripWhitespace)
x = tm_map(x, stemDocument)
x = tm_map(x, content_transformer(tolower))  # content_transformer required because tolower not tm function
x = tm_map(x, removeWords, stopwords('en'))
}
spam_path = '20021010_spam/'
# add spam to email corpus
# create initial corpus
email = readLines(str_c(spam_path, list.files(spam_path)[1]))
email = cleanse_emails(email)
email_corpus = Corpus(VectorSource(email))
email_corpus = cleanse_corpus(email_corpus)
meta(email_corpus[[1]], 'spam') = 1
for (i in 2:length(list.files(spam_path))) {
email = readLines(str_c(spam_path, list.files(spam_path)[i]))
email = cleanse_emails(email)
# add meta data to house spam classification
if (length(email) != 0) {
tmp_corpus = Corpus(VectorSource(email))
tmp_corpus = cleanse_corpus(tmp_corpus)
meta(tmp_corpus[[1]], 'spam') = 1
email_corpus = c(email_corpus, tmp_corpus)
}
}
easy_ham_path = '20021010_easy_ham/'
# add ham to email corpus
for (i in 1:length(list.files(easy_ham_path))) {
email = readLines(str_c(easy_ham_path, list.files(easy_ham_path)[i]))
email = cleanse_emails(email)
# add meta data to house spam classification
if (length(email) != 0) {
tmp_corpus = Corpus(VectorSource(email))
tmp_corpus = cleanse_corpus(tmp_corpus)
meta(tmp_corpus[[1]], 'spam') = 0
email_corpus = c(email_corpus, tmp_corpus)
}
}
hard_ham_path = '20021010_hard_ham/'
# add ham to email corpus
for (i in 1:length(list.files(hard_ham_path))) {
email = readLines(str_c(hard_ham_path, list.files(hard_ham_path)[i]))
email = cleanse_emails(email)
# add meta data to house spam classification
if (length(email) != 0) {
tmp_corpus = Corpus(VectorSource(email))
tmp_corpus = cleanse_corpus(tmp_corpus)
meta(tmp_corpus[[1]], 'spam') = 0
email_corpus = c(email_corpus, tmp_corpus)
}
}
# randomize corpus order
N = length(email_corpus)
rand_index = sample(1:N)
email_corpus = email_corpus[rand_index]
tm_filter(email_corpus[1:100], FUN = function(x) meta(x)[['spam']] == 1)  # check that spam and ham have been randomized
# create document term matrix
dtm = DocumentTermMatrix(email_corpus)
dtm = removeSparseTerms(dtm, 1-(10/length(email_corpus)))
#inspect(dtm[100:120, 100:110])
spam_labels = unlist(meta(email_corpus, 'spam'))
# create container
container = create_container(dtm, labels = spam_labels,
trainSize = 1:round(N*0.8,0), testSize = (round(N*0.8,0)+1):N,  # use 80% of data for train
virgin = FALSE)
# estimation procedure
svm_model = train_model(container, 'SVM')
#cross_validate(container, nfold = 5, algorithm = 'SVM')
tree_model = train_model(container, 'TREE')
#cross_validate(container, nfold = 5, algorithm = 'TREE')
maxent_model = train_model(container, 'MAXENT')
#cross_validate(container, nfold = 5, algorithm = 'MAXENT')
svm_out = classify_model(container, svm_model)
tree_out = classify_model(container, tree_model)
maxent_out = classify_model(container, maxent_model)
# evaluation
labels_out = data.frame(
correct_label = spam_labels[(round(N*0.8,0)+1):N],
svm = as.character(svm_out[,1]),
tree = as.character(tree_out[,1]),
maxent = as.character(maxent_out[,1]),
stringsAsFactors = FALSE
)
# svm confusion table
table(actual = labels_out[,1], predicted = labels_out[,2])
# tree confusion table
table(actual = labels_out[,1], predicted = labels_out[,3])
# maxent confusion table
table(actual = labels_out[,1], predicted = labels_out[,4])
# analytics
svm_analytics = create_analytics(container, svm_out)
summary(svm_analytics)
tree_analytics = create_analytics(container, tree_out)
summary(tree_analytics)
maxent_analytics = create_analytics(container, maxent_out)
summary(maxent_analytics)
svm_out %>% ggplot(aes(x = SVM_PROB, y = SVM_LABEL)) + geom_point()
# attempt modeling using caret package
library(caret)
library(parallel)
library(doParallel)  # for OSX use library(doMC)
library(caTools)
# convert dtm to matrix
full_mat = as.matrix(dtm)
train_mat = full_mat[1:round(N*0.8,0),]
test_mat = full_mat[(round(N*0.8,0)+1):N,]
# caret twoClassSummary requires non-numeric classes
spam_labels = factor(spam_labels, levels = c(0, 1), labels = c('ham', 'spam'))
train_y = spam_labels[1:round(N*0.8,0)]
test_y = spam_labels[(round(N*0.8,0)+1):N]
use_cores = detectCores()-1
cl = makeCluster(use_cores)
registerDoParallel(cl)  # for OSX use registerDoMC(cl)
# summaryFunction = twoClassSummary allows train() function to use AUC metric to rank models;
# classProbs = TRUE allows summaryFunction to work properly
myControl = trainControl(method = 'cv', number = 5, summaryFunction = twoClassSummary, classProbs = T, verboseIter = T)
# glmnet = a general linear model with combination of lasso regression (penalizes number of non-zero coefficients) and
# ridge regression (penalizes absolute magnitude of coefficients);
# glmnet has two tuning parameters: 1) alpha (0 to 1 or 100% lasso to 100% ridge) and 2) lambda (0 to Inf or the size
# of the penalty)
glm = train(x = train_mat, y = train_y, method = 'glmnet', metric = 'ROC', family = 'binomial', trControl = myControl)
#            tuneGrid = expand.grid(alpha = seq(0, 1, 0.1), lambda = seq(0, 0.1, 0.01)))
glm
plot(glm)
glm_pred = predict(glm, newdata = test_mat, type = 'raw')
colAUC(as.numeric(glm_pred), test_y, plotROC = T)
confusionMatrix(glm_pred, test_y)
# support vector machine
svm = train(x = train_mat, y = train_y, method = 'svmRadial', metric = 'ROC', family = 'binomial', trControl = myControl)
svm
plot(svm)
svm_pred = predict(svm, newdata = test_mat, type = 'raw')
colAUC(as.numeric(svm_pred), test_y, plotROC = T)
confusionMatrix(svm_pred, test_y)
# naive bayes -- FAILED
# nb = train(x = train_mat, y = train_y, method = 'nb', metric = 'ROC', family = 'binomial', trControl = myControl)
# plot(nb)
# nb_pred = predict(nb, newdata = test_mat, type = 'raw')
# colAUC(as.numeric(nb_pred), test_y, plotROC = T)
# random forest -- TOO SLOW
#rf = train(x = train_mat, y = train_y, method = 'rf', metric = 'ROC', family = 'binomial', trControl = myControl)
# ranger = random forest model which has one tuning parameter mtry (2 to 100) or the number of variables to consider
# at each split; this can be controlled by adjusting the tuneLength parameter or by defining a custom tuneGrid
# -- FAILED
#rf = train(x = train_mat, y = make.names(train_y), method = 'ranger', metric = 'ROC', trControl = myControl)
# neural network -- FAILED
#nnet = train(x = train_mat, y = train_y, method = 'nnet', metric = 'ROC', family = 'binomial', trControl = myControl)
stopCluster(cl)
registerDoSEQ()
library(caretEnsemble)
# compare models using resample
model_list = list(glm = glm, svm = svm)
resamps = resamples(model_list)
summary(resamps)
dotplot(resamps, metric = 'ROC')
setwd("C:/Users/bhao/Google Drive/CUNY/git/DATA606")
library(IS606)
startLab('Lab7')
setwd("C:/Users/bhao/Google Drive/CUNY/git/DATA606/Lab7")
9.41 * 0.67 / 10.37
b1 = y_sd * r / x_sd
x_bar = 107.2
x_sd = 10.37
y_bar = 171.14
y_sd = 9.41
r = 0.67
# slope
b1 = y_sd * r / x_sd
# intercept
b0 = b1 * x_bar - y_bar
b0
x_bar = 107.2
x_sd = 10.37
y_bar = 171.14
y_sd = 9.41
r = 0.67
# slope
b1 = y_sd * r / x_sd
b1
# intercept
b0 = b1 * x_bar - y_bar
b0
b0 = b1 * -x_bar + y_bar
x_bar = 107.2
x_sd = 10.37
y_bar = 171.14
y_sd = 9.41
r = 0.67
# slope
b1 = y_sd * r / x_sd
b1
# intercept
b0 = b1 * -x_bar + y_bar
b0
x_bar = 107.2
x_sd = 10.37
y_bar = 171.14
y_sd = 9.41
r = 0.67
# slope
b1 = y_sd * r / x_sd
b1
# intercept
b0 = y_bar - b1 * x_bar
b0
x_bar = 107.2
x_sd = 10.37
y_bar = 171.14
y_sd = 9.41
r = 0.67
# slope
b1 = y_sd * r / x_sd
b1
# intercept
b0 = y_bar - b1 * x_bar
b0
# r2
r2 = r^2
r2
x_bar = 107.2
x_sd = 10.37
y_bar = 171.14
y_sd = 9.41
r = 0.67
# slope
b1 = y_sd * r / x_sd
b1
# intercept
b0 = y_bar - b1 * x_bar
b0
# r2
r2 = r^2
r2
# predict height of student with 100cm shoulder girth
x = 100
y = b0 + 100 * b1
x_bar = 107.2
x_sd = 10.37
y_bar = 171.14
y_sd = 9.41
r = 0.67
# slope
b1 = y_sd * r / x_sd
b1
# intercept
b0 = y_bar - b1 * x_bar
b0
# r2
r2 = r^2
r2
# predict height of student with 100cm shoulder girth
x = 100
y = b0 + 100 * b1
y
# correlation coefficient
r2 = 0.6441
r = sqrt(r2)
# correlation coefficient
r2 = 0.6441
r = sqrt(r2)
r
x_bar = -0.0883
y_bar = 3.9983
b0 = 4.010
b1 = (y_bar - b0) / x_bar
b1
x_bar = -0.0883
y_bar = 3.9983
b0 = 4.010
b1 = (y_bar - b0) / x_bar
b1
0.0322 * 4.13
setwd("C:/Users/bhao/Google Drive/CUNY/git/DATA606/Lab7")
load(more/mlb11.RData)
load('more/mlb11.RData')
str(mlb11)
library(ggplot2)
mlb11 %>% ggplot(aes(x = at_bats, y = runs)) + geom_point()
library(dplyr)
setwd("C:/Users/bhao/Google Drive/CUNY/git/DATA606/Lab7")
library(IS606)
library(dplyr)
library(ggplot2)
load('more/mlb11.RData')
mlb11 %>% ggplot(aes(x = at_bats, y = runs)) + geom_point()
cor(mlb11$runs, mlb11$at_bats)
mlb11 %>% ggplot(aes(x = at_bats, y = runs)) + geom_point()
cor(mlb11$runs, mlb11$at_bats)
plot_ss(x = mlb11$at_bats, y = mlb11$runs)
plot_ss(x = mlb11$at_bats, y = mlb11$runs, showSquares = TRUE)
plot_ss(x = mlb11$at_bats, y = mlb11$runs, showSquares = TRUE)
plot_ss(x = mlb11$at_bats, y = mlb11$runs, showSquares = TRUE)
plot_ss(x = mlb11$at_bats, y = mlb11$runs, showSquares = TRUE)
plot_ss(x = mlb11$at_bats, y = mlb11$runs, showSquares = TRUE)
str(mlb11)
m2 = lm(runs ~ homeruns, data = mlb11)
summary(m2)
plot(mlb11$runs ~ mlb11$at_bats)
m1 <- lm(runs ~ at_bats, data = mlb11)
abline(m1)
mlb11[mlb11$at_bats == 5578, ]
y = -2789.2 + .06305 * 5578
y
m1 <- lm(runs ~ at_bats, data = mlb11)
summary(m1)
predict(m1, 5578)
predict(m1, data.frame(5578))
predict(m1, data.frame(at_bats = 5578))
m1 <- lm(runs ~ at_bats, data = mlb11)
summary(m1)
predict(m1, data.frame(at_bats = 5578))
y = -2789.2 + .6305 * 5578
y
m1 <- lm(runs ~ at_bats, data = mlb11)
summary(m1)
predict(m1, data.frame(at_bats = 5578))
y = -2789.2 + .6305 * 5578
y
m1 <- lm(runs ~ at_bats, data = mlb11)
summary(m1)
predict(m1, data.frame(at_bats = 5578))
mlb11 %>% arrange(at_bats)
plot(m1$residuals ~ mlb11$at_bats)
abline(h = 0, lty = 3)  # adds a horizontal dashed line at y = 0
plot(m1$residuals ~ mlb11$at_bats)
abline(h = 0, lty = 3)  # adds a horizontal dashed line at y = 0
plot(m1$residuals ~ mlb11$at_bats)
abline(h = 0, lty = 3)  # adds a horizontal dashed line at y = 0
hist(m1$residuals)
qqnorm(m1$residuals)
qqline(m1$residuals)  # adds diagonal line to the normal prob plot
mlb11 %>% ggplot(aes(x = homeruns, y = runs)) + geom_point()
plot_ss(x = mlb11$homeruns, y = mlb11$runs, showSquares = TRUE)
summary(m1)
summary(m2)
m1$model
m1$qr
summary(m2)
summary(m1)
summary(m2)
head(mlb11)
mlb11 %>% summarise(cor = cor(runs, at_bats))
mlb11 %>% summarise(at_bats = cor(runs, at_bats)^2)
mlb11 %>% summarise(at_bats = cor(runs, at_bats)^2, )
mlb11 %>% summarise(at_bats = cor(runs, at_bats)^2,
hits = cor(runs, hits)^2,
homeruns = cor(runs, homeruns)^2,
bat_avg = cor(runs, bat_avg)^2,
strikeouts = cor(runs, strikeouts)^2,
stolen_bases = cor(runs, stolen_bases)^2,
wins = cor(runs, wins)^2)
m3 = lm(runs ~ bat_avg, data = mlb11)
summary(m3)
str(mlb11)
mlb11 %>% summarise(new_onbase = cor(runs, new_onbase)^2,
new_slug = cor(runs, new_slug)^2,
new_obs = cor(runs, new_obs)^2)
m4 = lm(runs ~ new_obs, data = mlb11)
summary(m4)
hist(m4$residuals)
qqnorm(m4$residuals)
qqline(m4$residuals)  # adds diagonal line to the normal prob plot
m4 = lm(runs ~ new_obs, data = mlb11)
summary(m4)
plot(m4$residuals ~ mlb11$new_obs)
abline(h = 0, lty = 3)  # adds a horizontal dashed line at y = 0
hist(m4$residuals)
qqnorm(m4$residuals)
qqline(m4$residuals)  # adds diagonal line to the normal prob plot
mlb11 %>% ggplot(aes(x = at_bats, y = runs)) + geom_point()
cor(mlb11$runs, mlb11$at_bats)
