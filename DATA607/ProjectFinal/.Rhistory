if (length(email) != 0) {
tmp_corpus = Corpus(VectorSource(email))
tmp_corpus = cleanse_corpus(tmp_corpus)
meta(tmp_corpus[[1]], 'spam') = 1
email_corpus = c(email_corpus, tmp_corpus)
}
}
easy_ham_path = '20021010_easy_ham/'
# add ham to email corpus
for (i in 1:length(list.files(easy_ham_path))) {
email = readLines(str_c(easy_ham_path, list.files(easy_ham_path)[i]))
email = cleanse_emails(email)
# add meta data to house spam classification
if (length(email) != 0) {
tmp_corpus = Corpus(VectorSource(email))
tmp_corpus = cleanse_corpus(tmp_corpus)
meta(tmp_corpus[[1]], 'spam') = 0
email_corpus = c(email_corpus, tmp_corpus)
}
}
hard_ham_path = '20021010_hard_ham/'
# add ham to email corpus
for (i in 1:length(list.files(hard_ham_path))) {
email = readLines(str_c(hard_ham_path, list.files(hard_ham_path)[i]))
email = cleanse_emails(email)
# add meta data to house spam classification
if (length(email) != 0) {
tmp_corpus = Corpus(VectorSource(email))
tmp_corpus = cleanse_corpus(tmp_corpus)
meta(tmp_corpus[[1]], 'spam') = 0
email_corpus = c(email_corpus, tmp_corpus)
}
}
# randomize corpus order
N = length(email_corpus)
rand_index = sample(1:N)
email_corpus = email_corpus[rand_index]
tm_filter(email_corpus[1:100], FUN = function(x) meta(x)[['spam']] == 1)  # check that spam and ham have been randomized
# create document term matrix
dtm = DocumentTermMatrix(email_corpus)
dtm = removeSparseTerms(dtm, 1-(10/length(email_corpus)))
#inspect(dtm[100:120, 100:110])
spam_labels = unlist(meta(email_corpus, 'spam'))
# create container
container = create_container(dtm, labels = spam_labels,
trainSize = 1:round(N*0.8,0), testSize = (round(N*0.8,0)+1):N,  # use 80% of data for train
virgin = FALSE)
# estimation procedure
svm_model = train_model(container, 'SVM')
#cross_validate(container, nfold = 5, algorithm = 'SVM')
tree_model = train_model(container, 'TREE')
#cross_validate(container, nfold = 5, algorithm = 'TREE')
maxent_model = train_model(container, 'MAXENT')
#cross_validate(container, nfold = 5, algorithm = 'MAXENT')
svm_out = classify_model(container, svm_model)
tree_out = classify_model(container, tree_model)
maxent_out = classify_model(container, maxent_model)
# evaluation
labels_out = data.frame(
correct_label = spam_labels[(round(N*0.8,0)+1):N],
svm = as.character(svm_out[,1]),
tree = as.character(tree_out[,1]),
maxent = as.character(maxent_out[,1]),
stringsAsFactors = FALSE
)
# svm confusion table
table(actual = labels_out[,1], predicted = labels_out[,2])
# tree confusion table
table(actual = labels_out[,1], predicted = labels_out[,3])
# maxent confusion table
table(actual = labels_out[,1], predicted = labels_out[,4])
# analytics
svm_analytics = create_analytics(container, svm_out)
summary(svm_analytics)
tree_analytics = create_analytics(container, tree_out)
summary(tree_analytics)
maxent_analytics = create_analytics(container, maxent_out)
summary(maxent_analytics)
svm_out %>% ggplot(aes(x = SVM_PROB, y = SVM_LABEL)) + geom_point()
# attempt modeling using caret package
library(caret)
#library(parallel)
#library(doParallel)  # for OSX use library(doMC)
library(caTools)
# convert dtm to matrix
full_mat = as.matrix(dtm)
train_mat = full_mat[1:round(N*0.8,0),]
test_mat = full_mat[(round(N*0.8,0)+1):N,]
# caret twoClassSummary requires non-numeric classes
spam_labels = factor(spam_labels, levels = c(0, 1), labels = c('ham', 'spam'))
train_y = spam_labels[1:round(N*0.8,0)]
test_y = spam_labels[(round(N*0.8,0)+1):N]
#use_cores = detectCores()-1
#cl = makeCluster(use_cores)
#registerDoParallel(cl)  # for OSX use registerDoMC(cl)
# summaryFunction = twoClassSummary allows train() function to use AUC metric to rank models;
# classProbs = TRUE allows summaryFunction to work properly
myControl = trainControl(method = 'cv', number = 5, summaryFunction = twoClassSummary, classProbs = T, verboseIter = T)
# glmnet = a general linear model with combination of lasso regression (penalizes number of non-zero coefficients) and
# ridge regression (penalizes absolute magnitude of coefficients);
# glmnet has two tuning parameters: 1) alpha (0 to 1 or 100% lasso to 100% ridge) and 2) lambda (0 to Inf or the size
# of the penalty)
glm = train(x = train_mat, y = train_y, method = 'glmnet', metric = 'ROC', family = 'binomial', trControl = myControl)
install.packages("pROC")
library(tm)
library(stringr)
library(XML)
library(RTextTools)
library(dplyr)
library(ggplot2)
setwd("~/Google Drive/CUNY/git/DATA607/Week10")
#setwd("C:/Users/bhao/Google Drive/CUNY/git/DATA607/Week10")
set.seed(123)
# create function to cleanse emails by removing non ASCII characters
cleanse_emails = function(dat) {
dat = str_c(dat, collapse = " ")
dat2 <- unlist(strsplit(dat, split=" "))
dat3 <- grep("dat2", iconv(dat2, "latin1", "ASCII", sub="dat2"))
if (length(dat3) == 0) {
dat4 = dat2
} else dat4 = dat2[-dat3]
dat5 <- paste(dat4, collapse = " ")
}
cleanse_corpus = function(x) {
x = tm_map(x, PlainTextDocument)  # to make tm package play nice
x = tm_map(x, removeNumbers)
x = tm_map(x, removePunctuation)  # leaving punctuation in case relevant to spam ham filter
x = tm_map(x, stripWhitespace)
x = tm_map(x, stemDocument)
x = tm_map(x, content_transformer(tolower))  # content_transformer required because tolower not tm function
x = tm_map(x, removeWords, stopwords('en'))
}
spam_path = '20021010_spam/'
# add spam to email corpus
# create initial corpus
email = readLines(str_c(spam_path, list.files(spam_path)[1]))
email = cleanse_emails(email)
email_corpus = Corpus(VectorSource(email))
email_corpus = cleanse_corpus(email_corpus)
meta(email_corpus[[1]], 'spam') = 1
for (i in 2:length(list.files(spam_path))) {
email = readLines(str_c(spam_path, list.files(spam_path)[i]))
email = cleanse_emails(email)
# add meta data to house spam classification
if (length(email) != 0) {
tmp_corpus = Corpus(VectorSource(email))
tmp_corpus = cleanse_corpus(tmp_corpus)
meta(tmp_corpus[[1]], 'spam') = 1
email_corpus = c(email_corpus, tmp_corpus)
}
}
easy_ham_path = '20021010_easy_ham/'
# add ham to email corpus
for (i in 1:length(list.files(easy_ham_path))) {
email = readLines(str_c(easy_ham_path, list.files(easy_ham_path)[i]))
email = cleanse_emails(email)
# add meta data to house spam classification
if (length(email) != 0) {
tmp_corpus = Corpus(VectorSource(email))
tmp_corpus = cleanse_corpus(tmp_corpus)
meta(tmp_corpus[[1]], 'spam') = 0
email_corpus = c(email_corpus, tmp_corpus)
}
}
hard_ham_path = '20021010_hard_ham/'
# add ham to email corpus
for (i in 1:length(list.files(hard_ham_path))) {
email = readLines(str_c(hard_ham_path, list.files(hard_ham_path)[i]))
email = cleanse_emails(email)
# add meta data to house spam classification
if (length(email) != 0) {
tmp_corpus = Corpus(VectorSource(email))
tmp_corpus = cleanse_corpus(tmp_corpus)
meta(tmp_corpus[[1]], 'spam') = 0
email_corpus = c(email_corpus, tmp_corpus)
}
}
# randomize corpus order
N = length(email_corpus)
rand_index = sample(1:N)
email_corpus = email_corpus[rand_index]
tm_filter(email_corpus[1:100], FUN = function(x) meta(x)[['spam']] == 1)  # check that spam and ham have been randomized
# create document term matrix
dtm = DocumentTermMatrix(email_corpus)
dtm = removeSparseTerms(dtm, 1-(10/length(email_corpus)))
#inspect(dtm[100:120, 100:110])
spam_labels = unlist(meta(email_corpus, 'spam'))
# create container
container = create_container(dtm, labels = spam_labels,
trainSize = 1:round(N*0.8,0), testSize = (round(N*0.8,0)+1):N,  # use 80% of data for train
virgin = FALSE)
# estimation procedure
svm_model = train_model(container, 'SVM')
#cross_validate(container, nfold = 5, algorithm = 'SVM')
tree_model = train_model(container, 'TREE')
#cross_validate(container, nfold = 5, algorithm = 'TREE')
maxent_model = train_model(container, 'MAXENT')
#cross_validate(container, nfold = 5, algorithm = 'MAXENT')
svm_out = classify_model(container, svm_model)
tree_out = classify_model(container, tree_model)
maxent_out = classify_model(container, maxent_model)
# evaluation
labels_out = data.frame(
correct_label = spam_labels[(round(N*0.8,0)+1):N],
svm = as.character(svm_out[,1]),
tree = as.character(tree_out[,1]),
maxent = as.character(maxent_out[,1]),
stringsAsFactors = FALSE
)
# svm confusion table
table(actual = labels_out[,1], predicted = labels_out[,2])
# tree confusion table
table(actual = labels_out[,1], predicted = labels_out[,3])
# maxent confusion table
table(actual = labels_out[,1], predicted = labels_out[,4])
# analytics
svm_analytics = create_analytics(container, svm_out)
summary(svm_analytics)
tree_analytics = create_analytics(container, tree_out)
summary(tree_analytics)
maxent_analytics = create_analytics(container, maxent_out)
summary(maxent_analytics)
svm_out %>% ggplot(aes(x = SVM_PROB, y = SVM_LABEL)) + geom_point()
# attempt modeling using caret package
library(caret)
#library(parallel)
#library(doParallel)  # for OSX use library(doMC)
library(caTools)
# convert dtm to matrix
full_mat = as.matrix(dtm)
train_mat = full_mat[1:round(N*0.8,0),]
test_mat = full_mat[(round(N*0.8,0)+1):N,]
# caret twoClassSummary requires non-numeric classes
spam_labels = factor(spam_labels, levels = c(0, 1), labels = c('ham', 'spam'))
train_y = spam_labels[1:round(N*0.8,0)]
test_y = spam_labels[(round(N*0.8,0)+1):N]
#use_cores = detectCores()-1
#cl = makeCluster(use_cores)
#registerDoParallel(cl)  # for OSX use registerDoMC(cl)
# summaryFunction = twoClassSummary allows train() function to use AUC metric to rank models;
# classProbs = TRUE allows summaryFunction to work properly
myControl = trainControl(method = 'cv', number = 5, summaryFunction = twoClassSummary, classProbs = T, verboseIter = T)
# glmnet = a general linear model with combination of lasso regression (penalizes number of non-zero coefficients) and
# ridge regression (penalizes absolute magnitude of coefficients);
# glmnet has two tuning parameters: 1) alpha (0 to 1 or 100% lasso to 100% ridge) and 2) lambda (0 to Inf or the size
# of the penalty)
glm = train(x = train_mat, y = train_y, method = 'glmnet', metric = 'ROC', family = 'binomial', trControl = myControl)
#            tuneGrid = expand.grid(alpha = seq(0, 1, 0.1), lambda = seq(0, 0.1, 0.01)))
glm
plot(glm)
glm_pred = predict(glm, newdata = test_mat, type = 'raw')
colAUC(as.numeric(glm_pred), test_y, plotROC = T)
confusionMatrix(glm_pred, test_y)
# support vector machine
svm = train(x = train_mat, y = train_y, method = 'svmRadial', metric = 'ROC', family = 'binomial', trControl = myControl)
svm
plot(svm)
svm_pred = predict(svm, newdata = test_mat, type = 'raw')
colAUC(as.numeric(svm_pred), test_y, plotROC = T)
confusionMatrix(svm_pred, test_y)
# naive bayes -- FAILED
# nb = train(x = train_mat, y = train_y, method = 'nb', metric = 'ROC', family = 'binomial', trControl = myControl)
# plot(nb)
# nb_pred = predict(nb, newdata = test_mat, type = 'raw')
# colAUC(as.numeric(nb_pred), test_y, plotROC = T)
# random forest -- TOO SLOW
#rf = train(x = train_mat, y = train_y, method = 'rf', metric = 'ROC', family = 'binomial', trControl = myControl)
# ranger = random forest model which has one tuning parameter mtry (2 to 100) or the number of variables to consider
# at each split; this can be controlled by adjusting the tuneLength parameter or by defining a custom tuneGrid
# -- FAILED
#rf = train(x = train_mat, y = make.names(train_y), method = 'ranger', metric = 'ROC', trControl = myControl)
# neural network -- FAILED
#nnet = train(x = train_mat, y = train_y, method = 'nnet', metric = 'ROC', family = 'binomial', trControl = myControl)
#stopCluster(cl)
#registerDoSEQ()
library(caretEnsemble)
install.packages("caretEnsemble")
library(caretEnsemble)
# compare models using resample
model_list = list(glm = glm, svm = svm)
resamps = resamples(model_list)
summary(resamps)
dotplot(resamps, metric = 'ROC')
setwd("~/Google Drive/CUNY/git/DATA607/Project4Final")
install.packages("recommenderlab")
library(recommenderlab)
path_name = "~/Google Drive/CUNY/git/DATA607/Project4Final"
?read.csv
ratings = read.csv(paste0(path_name, '/ml-latest-small/ratings.csv'), stringsAsFactors = FALSE)
head(ratings)
library(dplyr)
library(dplyr)
library(ggplot2)
library(ggthemes)
ratings %>% ggplot(aes(x = rating)) +
geom_histogram()
ratings %>% ggplot(aes(x = rating)) +
geom_histogram(bins = 10)
ratings %>% ggplot(aes(x = rating)) +
geom_histogram(bins = 10) +
scale_fill_economist()
ratings %>% ggplot(aes(x = rating)) +
geom_histogram(bins = 10) +
scale_fill_economist() + scale_color_economist()
ratings %>% ggplot(aes(x = rating)) +
geom_histogram(bins = 10) +
scale_fill_economist() + scale_color_economist()
ratings %>% ggplot(aes(x = rating)) +
geom_histogram(bins = 10) +
ggtitle('Histogram of Movie Ratings') +
scale_fill_economist() +
scale_color_economist()
ratings %>% ggplot(aes(x = rating)) +
geom_point() +
#  geom_histogram(bins = 10) +
ggtitle('Histogram of Movie Ratings') +
scale_fill_economist() +
scale_color_economist()
ratings %>% ggplot(aes(x = rating, y = userId)) +
geom_point() +
#  geom_histogram(bins = 10) +
ggtitle('Histogram of Movie Ratings') +
scale_fill_economist() +
scale_color_economist()
ratings %>% ggplot(aes(x = rating, col = 1)) +
geom_histogram(bins = 10) +
ggtitle('Histogram of Movie Ratings') +
scale_fill_economist() +
scale_color_economist()
ratings %>% ggplot(aes(x = rating, col = 'abc')) +
geom_histogram(bins = 10) +
ggtitle('Histogram of Movie Ratings') +
scale_fill_economist() +
scale_color_economist()
abc
ratings %>% ggplot(aes(x = rating, fill = 1)) +
geom_histogram(bins = 10) +
ggtitle('Histogram of Movie Ratings') +
scale_fill_economist() +
scale_color_economist()
ratings %>% ggplot(aes(x = rating, fill = '1')) +
geom_histogram(bins = 10) +
ggtitle('Histogram of Movie Ratings') +
scale_fill_economist() +
scale_color_economist()
ratings %>% ggplot(aes(x = rating, fill = 'ratings')) +
geom_histogram(bins = 10) +
ggtitle('Histogram of Movie Ratings') +
scale_fill_economist() +
scale_color_economist()
ratings %>% ggplot(aes(x = rating)) +
geom_histogram(bins = 10) +
ggtitle('Histogram of Movie Ratings') +
theme_economist()
ratings %>% ggplot(aes(x = rating)) +
geom_histogram(bins = 10) +
ggtitle('Histogram of Movie Ratings') +
theme_minimal()
ratings %>% ggplot(aes(x = rating)) +
geom_histogram(bins = 10) +
ggtitle('Histogram of Movie Ratings')
head(ratings)
ratings %>% group_by(userId) %>%
summarise(avgRating = mean(rating), nRatings = n(rating))
ratings %>% group_by(userId) %>%
summarise(avgRating = mean(rating), nRatings = n())
ratings %>% group_by(userId) %>%
summarise(avgRating = mean(rating), nRatings = n()) %>%
ggplot(aes(x = avgRating)) +
geom_histogram()
ratings %>% group_by(userId) %>%
summarise(avgRating = mean(rating), nRatings = n()) %>%
ggplot(aes(x = nRatings)) +
geom_histogram()
multiplot(p1, p2, p3, p4, col = 2)
library(grid)
library(gridExtra)
grid.arrange(p1, p2, p3, p4, ncol = 2)
ratings %>% ggplot(aes(x = rating)) +
geom_histogram(bins = 10) +
ggtitle('Histogram of Movie Ratings')
p1 = ratings %>% group_by(userId) %>%
summarise(avgRating = mean(rating), nRatings = n()) %>%
ggplot(aes(x = avgRating)) +
geom_histogram()
p2 = ratings %>% group_by(userId) %>%
summarise(avgRating = mean(rating), nRatings = n()) %>%
ggplot(aes(x = nRatings)) +
geom_histogram()
p3 = ratings %>% group_by(movieId) %>%
summarise(avgRating = mean(rating), nRatings = n()) %>%
ggplot(aes(x = avgRating)) +
geom_histogram()
p4 = ratings %>% group_by(movieId) %>%
summarise(avgRating = mean(rating), nRatings = n()) %>%
ggplot(aes(x = nRating)) +
geom_histogram()
grid.arrange(p1, p2, p3, p4, ncol = 2)
p4 = ratings %>% group_by(movieId) %>%
summarise(avgRating = mean(rating), nRatings = n()) %>%
ggplot(aes(x = nRatings)) +
geom_histogram()
grid.arrange(p1, p2, p3, p4, ncol = 2)
p1 = ratings %>% group_by(userId) %>%
summarise(avgRating = mean(rating), nRatings = n()) %>%
ggplot(aes(x = avgRating)) +
geom_histogram() +
xlab('User Avg Rating')
grid.arrange(p1, p2, p3, p4, ncol = 2)
ratings %>% ggplot(aes(x = rating)) +
geom_histogram(bins = 10) +
ggtitle('Histogram of Movie Ratings')
p1 = ratings %>% group_by(userId) %>%
summarise(avgRating = mean(rating), nRatings = n()) %>%
ggplot(aes(x = avgRating)) +
geom_histogram() +
xlab('User Average Rating')
p2 = ratings %>% group_by(userId) %>%
summarise(avgRating = mean(rating), nRatings = n()) %>%
ggplot(aes(x = nRatings)) +
geom_histogram() +
xlab('Number of Rated Movies')
p3 = ratings %>% group_by(movieId) %>%
summarise(avgRating = mean(rating), nRatings = n()) %>%
ggplot(aes(x = avgRating)) +
geom_histogram() +
xlab('Movie Average Rating')
p4 = ratings %>% group_by(movieId) %>%
summarise(avgRating = mean(rating), nRatings = n()) %>%
ggplot(aes(x = nRatings)) +
geom_histogram() +
xlab('Number of Ratings per Movie')
grid.arrange(p1, p2, p3, p4, ncol = 2)
ratings %>% ggplot(aes(x = rating)) +
geom_histogram(bins = 10) +
ggtitle('Histogram of Movie Ratings')
p1 = ratings %>% group_by(userId) %>%
summarise(avgRating = mean(rating), nRatings = n()) %>%
ggplot(aes(x = avgRating)) +
geom_histogram() +
xlab('User Average Rating')
p2 = ratings %>% group_by(userId) %>%
summarise(avgRating = mean(rating), nRatings = n()) %>%
ggplot(aes(x = nRatings)) +
geom_histogram() +
xlab('Number of Rated Movies') +
ylab('Users')
p3 = ratings %>% group_by(movieId) %>%
summarise(avgRating = mean(rating), nRatings = n()) %>%
ggplot(aes(x = avgRating)) +
geom_histogram() +
xlab('Movie Average Rating')
p4 = ratings %>% group_by(movieId) %>%
summarise(avgRating = mean(rating), nRatings = n()) %>%
ggplot(aes(x = nRatings)) +
geom_histogram() +
xlab('Number of Ratings per Movie') +
ylab('Movies')
grid.arrange(p1, p3, p2, p4, ncol = 2)
library(recommenderlab)
library(dplyr)
library(ggplot2)
library(ggthemes)
library(gridExtra)
setwd("~/Google Drive/CUNY/git/DATA607/Project4Final")
path_name = "~/Google Drive/CUNY/git/DATA607/Project4Final"
ratings = read.csv(paste0(path_name, '/ml-latest-small/ratings.csv'), stringsAsFactors = FALSE)
sparse_matrix = sparseMatrix(i = ratings$userId, j = ratings$movieId, x = ratings$rating,
dims = c(length(unique(ratings$userId))), length(unique(ratings$movieId)),
dimnames = list(paste0('u', 1:length(unique(ratings$userId))),
paste0('m', 1:length(unique(ratings$movieId)))))
sparse_matrix = sparseMatrix(i = ratings$userId, j = ratings$movieId, x = ratings$rating,
dims = c(length(unique(ratings$userId)), length(unique(ratings$movieId))),
dimnames = list(paste0('u', 1:length(unique(ratings$userId))),
paste0('m', 1:length(unique(ratings$movieId)))))
paste0('u', 1:length(unique(ratings$userId))
)
length(unique(ratings$userId))
length(unique(ratings$movieId))
paste0('m', 1:length(unique(ratings$movieId)))
sparse_matrix = sparseMatrix(i = ratings$userId, j = ratings$movieId, x = ratings$rating,
dims = c(length(unique(ratings$userId)), length(unique(ratings$movieId))),
dimnames = list(paste0('u', 1:length(unique(ratings$userId))),
paste0('m', 1:length(unique(ratings$movieId)))))
sparse_matrix = sparseMatrix(i = ratings$userId, j = ratings$movieId, x = ratings$rating,
dims = c(length(unique(ratings$userId)), length(unique(ratings$movieId))))
sparse_matrix = sparseMatrix(i = ratings$userId, j = ratings$movieId, x = ratings$rating)
sparse_matrix[1:10, 1:10]
nrow(sparse_matrix)
ncol(sparse_matrix)
dim(sparse_matrix)
sparse_matrix = sparseMatrix(i = unique(ratings$userId), j = unique(ratings$movieId), x = ratings$rating,
dims = c(length(unique(ratings$userId)), length(unique(ratings$movieId))),
dimnames = list(paste0('u', 1:length(unique(ratings$userId))),
paste0('m', 1:length(unique(ratings$movieId)))))
sparse_matrix = sparseMatrix(i = ratings$userId, j = unique(ratings$movieId), x = ratings$rating)
sparse_matrix = sparseMatrix(i = unique(ratings$userId), j = unique(ratings$movieId), x = ratings$rating)
?sparseMatrix
sparse_matrix = sparseMatrix(i = unique(ratings$userId), j = unique(ratings$movieId), x = ratings$rating,
dims = c(length(unique(ratings$userId)), length(unique(ratings$movieId))),
dimnames = list(paste0('u', 1:length(unique(ratings$userId))),
paste0('m', 1:length(unique(ratings$movieId)))))
sparse_matrix = sparseMatrix(i = ratings$userId, j = ratings$movieId, x = ratings$rating),
sparse_matrix = sparseMatrix(i = ratings$userId, j = ratings$movieId, x = ratings$rating,
dimnames = list(paste0('u', 1:length(unique(ratings$userId))),
paste0('m', 1:length(unique(ratings$movieId)))))
sparse_matrix = sparseMatrix(i = unique(ratings$userId), j = unique(ratings$movieId), x = ratings$rating,
dims = c(length(unique(ratings$userId)), length(unique(ratings$movieId))))
short_ratings = ratings[1:100, ]
str(short_ratings)
sparse_matrix = sparseMatrix(i = unique(short_ratings$userId), j = unique(short_ratings$movieId), x = short_ratings$rating,
dims = c(length(unique(short_ratings$userId)), length(unique(short_ratings$movieId))))
