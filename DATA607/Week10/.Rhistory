trainSize = train_index, testSize = test_index,
virgin = FALSE)
slotNames(container)
container@column_names
container@training_codes
cross_validate(container, nfold = 10, algorithm = 'SVM')
cross_validate(container, nfold = 10, algorithm = 'TREE')
cross_validate(container, nfold = 10, algorithm = 'MAXENT')
tree_model = train_model(container, 'TREE')
tree_out = classify_model(container, tree_model)
labels_out = data.frame(
correct_label = spam_labels[test_index],
tree = as.character(tree_out[,1]),
stringsAsFactors = FALSE
)
table(labels_out[,1] == labels_out[,2])
prop.table(table(labels_out[,1] == labels_out[,2]))
cross_validate(container, nfold = 10, algorithm = 'TREE')
maxent_model = train_model(container, 'MAXENT')
i
svm_model = train_model(container, 'SVM')
tree_model = train_model(container, 'TREE')
maxent_model = train_model(container, 'MAXENT')
maxent_model = train_model(container, 'MAXENT')
svm_out = classify_model(container, svm_model)
tree_out = classify_model(container, tree_model)
maxent_out = classify_model(container, maxent_model)
set.seed(123)
# evaluation
head(svm_out)
labels_out = data.frame(
correct_label = spam_labels[401:N],
svm = as.character(svm_out[,1]),
tree = as.character(tree_out[,1]),
maxent = as.character(maxent_out[,1]),
stringsAsFactors = FALSE
)
# svm performance
table(labels_out[,1] == labels_out[,2])
prop.table(table(labels_out[,1] == labels_out[,2]))
# random forest performance
table(labels_out[,1] == labels_out[,3])
prop.table(table(labels_out[,1] == labels_out[,3]))
# maximum entropy performance
table(labels_out[,1] == labels_out[,4])
prop.table(table(labels_out[,1] == labels_out[,4]))
labels_out = data.frame(
correct_label = spam_labels[401:N],
svm = as.character(svm_out[,1]),
tree = as.character(tree_out[,1]),
maxent = as.character(maxent_out[,1]),
stringsAsFactors = FALSE
)
labels_out = data.frame(
correct_label = spam_labels[test_index],
svm = as.character(svm_out[,1]),
tree = as.character(tree_out[,1]),
maxent = as.character(maxent_out[,1]),
stringsAsFactors = FALSE
)
table(labels_out[,1] == labels_out[,2])
prop.table(table(labels_out[,1] == labels_out[,2]))
# random forest performance
table(labels_out[,1] == labels_out[,3])
prop.table(table(labels_out[,1] == labels_out[,3]))
# maximum entropy performance
table(labels_out[,1] == labels_out[,4])
prop.table(table(labels_out[,1] == labels_out[,4]))
cross_validate(container, nfold = 10, algorithm = 'MAXENT')
?cross_validate
cross_validate(container, nfold = 10, algorithm = 'SLDA')
cross_validate(container, nfold = 10, algorithm = 'BOOSTING')
table(labels_out[,1], labels_out[,2])
summary(labels_out[,1])
str(labels_out[,1])
table(labels_out[,1])
table(labels_out[,2])
table(labels_out[,3])
table(labels_out[,4])
dtm$i
dtm$nrow
dtm$dimnames
library(tm)
library(stringr)
library(XML)
library(RTextTools)
setwd("~/Google Drive/CUNY/git/DATA607/Week10")
#setwd("C:/Users/bhao/Google Drive/CUNY/git/DATA607/Week10")
set.seed(123)
# create function to cleanse emails
# cleanse_emails = function(dat) {
#   dat = str_c(x, collapse = " ")
#   dat2 <- unlist(strsplit(dat, split=" "))
#   dat3 <- grep("dat2", iconv(dat2, "latin1", "ASCII", sub="dat2"))
#   dat4 <- dat2[-dat3]
#   dat5 <- paste(dat4, collapse = ", ")
# }
cleanse_corpus = function(x) {
x = tm_map(x, PlainTextDocument)  # to make tm package play nice
x = tm_map(x, removeNumbers)
x = tm_map(x, removePunctuation)  # leaving punctuation in case relevant to spam ham filter
x = tm_map(x, stripWhitespace)
x = tm_map(x, stemDocument)
x = tm_map(x, content_transformer(tolower))  # content_transformer required because tolower not tm function
x = tm_map(x, removeWords, stopwords('en'))
}
spam_path = '20021010_spam/'
# add spam to email corpus
# create initial corpus
email = readLines(str_c(spam_path, list.files(spam_path)[1]), encoding = 'UTF-8')
email = str_c(email, collapse = " ")
email_corpus = Corpus(VectorSource(email))
email_corpus = cleanse_corpus(email_corpus)
meta(email_corpus[[1]], 'spam') = 1
for (i in 2:length(list.files(spam_path))) {
email = readLines(str_c(spam_path, list.files(spam_path)[i]), encoding = 'UTF-8')
email = str_c(email, collapse = " ")
# add meta data to house spam classification
if (length(email) != 0) {
tmp_corpus = Corpus(VectorSource(email))
tmp_corpus = cleanse_corpus(tmp_corpus)
meta(tmp_corpus[[1]], 'spam') = 1
email_corpus = c(email_corpus, tmp_corpus)
}
}
cleanse_emails = function(x) {
x = str_c(x, collapse = " ")
Encoding(x) = 'UTF-8'
iconv(x, 'UTF-8', 'UTF-8', sub = '')
}
spam_path = '20021010_spam/'
# add spam to email corpus
# create initial corpus
email = readLines(str_c(spam_path, list.files(spam_path)[1]), encoding = 'UTF-8')
#email = str_c(email, collapse = " ")
email = cleanse_emails(email)
email_corpus = Corpus(VectorSource(email))
email_corpus = cleanse_corpus(email_corpus)
meta(email_corpus[[1]], 'spam') = 1
for (i in 2:length(list.files(spam_path))) {
email = readLines(str_c(spam_path, list.files(spam_path)[i]), encoding = 'UTF-8')
#email = str_c(email, collapse = " ")
email = cleanse_emails(email)
# add meta data to house spam classification
if (length(email) != 0) {
tmp_corpus = Corpus(VectorSource(email))
tmp_corpus = cleanse_corpus(tmp_corpus)
meta(tmp_corpus[[1]], 'spam') = 1
email_corpus = c(email_corpus, tmp_corpus)
}
}
email = readLines(str_c(spam_path, list.files(spam_path)[1]), encoding = 'UTF-8')
email
cleanse_emails = function(x) {
Encoding(x) = 'UTF-8'
iconv(x, 'UTF-8', 'UTF-8', sub = '')
x = str_c(x, collapse = " ")
}
spam_path = '20021010_spam/'
# add spam to email corpus
# create initial corpus
email = readLines(str_c(spam_path, list.files(spam_path)[1]), encoding = 'UTF-8')
#email = str_c(email, collapse = " ")
email = cleanse_emails(email)
email_corpus = Corpus(VectorSource(email))
email_corpus = cleanse_corpus(email_corpus)
meta(email_corpus[[1]], 'spam') = 1
for (i in 2:length(list.files(spam_path))) {
email = readLines(str_c(spam_path, list.files(spam_path)[i]), encoding = 'UTF-8')
#email = str_c(email, collapse = " ")
email = cleanse_emails(email)
# add meta data to house spam classification
if (length(email) != 0) {
tmp_corpus = Corpus(VectorSource(email))
tmp_corpus = cleanse_corpus(tmp_corpus)
meta(tmp_corpus[[1]], 'spam') = 1
email_corpus = c(email_corpus, tmp_corpus)
}
}
cleanse_corpus = function(x) {
x = tm_map(x, PlainTextDocument)  # to make tm package play nice
x = tm_map(x, removeNumbers)
x = tm_map(x, removePunctuation)  # leaving punctuation in case relevant to spam ham filter
x = tm_map(x, stripWhitespace)
x = tm_map(x, stemDocument)
x = tm_map(x, content_transformer(tolower))  # content_transformer required because tolower not tm function
x = sapply(x, function(row) iconv(row, "latin1", "ASCII", sub=""))
x = tm_map(x, removeWords, stopwords('en'))
}
spam_path = '20021010_spam/'
# add spam to email corpus
# create initial corpus
email = readLines(str_c(spam_path, list.files(spam_path)[1]), encoding = 'UTF-8')
email = str_c(email, collapse = " ")
email_corpus = Corpus(VectorSource(email))
email_corpus = cleanse_corpus(email_corpus)
dat <- "Special,  satisfação, Happy, Sad, Potential, für"
dat2 <- unlist(strsplit(dat, split=", "))
dat2
dat <- "Special  satisfação Happy Sad Potential für"
dat2 <- unlist(strsplit(dat, split=", "))
dat2
dat3 <- grep("dat2", iconv(dat2, "latin1", "ASCII", sub="dat2"))
dat3
dat <- "Special,  satisfação, Happy, Sad, Potential, für"
dat2 <- unlist(strsplit(dat, split=", "))
dat3 <- grep("dat2", iconv(dat2, "latin1", "ASCII", sub="dat2"))
dat3
dat4 <- dat2[-dat3]
dat4
dat5 <- paste(dat4, collapse = ", ")
dat5
dat2 <- unlist(strsplit(dat, split=" "))
dat <- "Special  satisfação Happy Sad Potential für"
dat2 <- unlist(strsplit(dat, split=" "))
dat2
dat3 <- grep("dat2", iconv(dat2, "latin1", "ASCII", sub="dat2"))
dat4 <- dat2[-dat3]
dat5 <- paste(dat4, collapse = ", ")
dat5
dat <- "Special  satisfação Happy Sad Potential für"
dat2 <- unlist(strsplit(dat, split=" "))
dat3 <- grep("dat2", iconv(dat2, "latin1", "ASCII", sub="dat2"))
dat4 <- dat2[-dat3]
dat5 <- paste(dat4, collapse = " ")
dat5
cleanse_emails = function(dat) {
dat = str_c(x, collapse = " ")
dat2 <- unlist(strsplit(dat, split=" "))
dat3 <- grep("dat2", iconv(dat2, "latin1", "ASCII", sub="dat2"))
dat4 <- dat2[-dat3]
dat5 <- paste(dat4, collapse = ", ")
}
cleanse_emails = function(dat) {
dat = str_c(x, collapse = " ")
dat2 <- unlist(strsplit(dat, split=" "))
dat3 <- grep("dat2", iconv(dat2, "latin1", "ASCII", sub="dat2"))
dat4 <- dat2[-dat3]
dat5 <- paste(dat4, collapse = " ")
}
email = readLines(str_c(spam_path, list.files(spam_path)[38]))
getwd()
setwd("~/Google Drive/CUNY/git/DATA607/Week10")
email = readLines(str_c(spam_path, list.files(spam_path)[38]))
email
x = email
dat = email
dat = str_c(dat, collapse = " ")
dat
dat2 <- unlist(strsplit(dat, split=" "))
dat2
dat3 <- grep("dat2", iconv(dat2, "latin1", "ASCII", sub="dat2"))
dat3
length(dat3)
dat2[108]
dat2[113]
dat2[1]
dat2
dat3
dat4 <- dat2[-dat3]
dat4
dat5 <- paste(dat4, collapse = " ")
dat5
cleanse_emails = function(dat) {
dat = str_c(dat, collapse = " ")
dat2 <- unlist(strsplit(dat, split=" "))
dat3 <- grep("dat2", iconv(dat2, "latin1", "ASCII", sub="dat2"))
dat4 <- dat2[-dat3]
dat5 <- paste(dat4, collapse = " ")
}
spam_path = '20021010_spam/'
# add spam to email corpus
# create initial corpus
email = readLines(str_c(spam_path, list.files(spam_path)[1]))
email = cleanse_emails(email)
email_corpus = Corpus(VectorSource(email))
email_corpus = cleanse_corpus(email_corpus)
spam_path = '20021010_spam/'
# add spam to email corpus
# create initial corpus
email = readLines(str_c(spam_path, list.files(spam_path)[1]))
email = cleanse_emails(email)
email_corpus = Corpus(VectorSource(email))
email_corpus = cleanse_corpus(email_corpus)
email = readLines(str_c(spam_path, list.files(spam_path)[1]))
email = cleanse_emails(email)
email
email = readLines(str_c(spam_path, list.files(spam_path)[1]))
email
cleanse_emails = function(dat) {
dat = str_c(dat, collapse = " ")
dat2 <- unlist(strsplit(dat, split=" "))
dat3 <- grep("dat2", iconv(dat2, "latin1", "ASCII", sub="dat2"))
dat4 <- dat2[-dat3]
dat5 <- paste(dat4, collapse = " ")
return(dat5)
}
email = cleanse_emails(email)
email
email = readLines(str_c(spam_path, list.files(spam_path)[1]))
dat = email
dat = str_c(dat, collapse = " ")
dat
dat2 <- unlist(strsplit(dat, split=" "))
dat2
dat3 <- grep("dat2", iconv(dat2, "latin1", "ASCII", sub="dat2"))
dat3
dat4 <- dat2[-dat3]
dat4
dat3 == 0
dat3 == TRUE
dat4 <- ifelse(dat3 == 0, dat2, dat2[-dat3])
dat4
dat3
class(dat3)
as.numeric(dat3)
as.numeric(dat3) == 0
length(dat3)
dat4 <- ifelse(length(dat3) == 0, dat2, dat2[-dat3])
dat4
dat = email
dat = str_c(dat, collapse = " ")
dat2 <- unlist(strsplit(dat, split=" "))
dat3 <- grep("dat2", iconv(dat2, "latin1", "ASCII", sub="dat2"))
dat2
dat4 <- ifelse(length(dat3) == 0, dat2, dat2[-dat3])
dat4
length(dat3)==0
dat2
str(dat4)
dat2
dat2 == dat4
dat3 <- grep("dat2", iconv(dat2, "latin1", "ASCII", sub="dat2"))
if (length(dat3) == 0) {
dat4 = dat2
}
else {dat4 = dat2[-dat3]
if (length(dat3) == 0) { dat4 = dat2 }
else { dat4 = dat2[-dat3] }
if (length(dat3) == 0) { dat4 = dat2 }
else dat4 = dat2[-dat3]
dat4 = if (length(dat3) == 0) { dat2
dat4 = if (length(dat3) == 0) { dat2
} else dat2[-dat3]
if (length(dat3) == 0) {
dat4 = dat2
} else dat4 = dat2[-dat3]
dat4
dat5 <- paste(dat4, collapse = " ")
dat5
cleanse_emails = function(dat) {
dat = str_c(dat, collapse = " ")
dat2 <- unlist(strsplit(dat, split=" "))
dat3 <- grep("dat2", iconv(dat2, "latin1", "ASCII", sub="dat2"))
if (length(dat3) == 0) {
dat4 = dat2
} else dat4 = dat2[-dat3]
dat5 <- paste(dat4, collapse = " ")
}
spam_path = '20021010_spam/'
# add spam to email corpus
# create initial corpus
email = readLines(str_c(spam_path, list.files(spam_path)[1]))
email = cleanse_emails(email)
email_corpus = Corpus(VectorSource(email))
email_corpus = cleanse_corpus(email_corpus)
email
email_corpus
email = readLines(str_c(spam_path, list.files(spam_path)[1]))
email = cleanse_emails(email)
email_corpus = Corpus(VectorSource(email))
email_corpus = cleanse_corpus(email_corpus)
email_corpus[[1]]
email_corpus[[1]]$content
x = email_corpus
x = tm_map(x, PlainTextDocument)  # to make tm package play nice
x = tm_map(x, removeNumbers)
x = tm_map(x, removePunctuation)  # leaving punctuation in case relevant to spam ham filter
x = tm_map(x, stripWhitespace)
x = tm_map(x, stemDocument)
x = tm_map(x, content_transformer(tolower))  # content_transformer required because tolower not tm function
x = tm_map(x, removeWords, stopwords('en'))
cleanse_corpus = function(x) {
x = tm_map(x, PlainTextDocument)  # to make tm package play nice
x = tm_map(x, removeNumbers)
x = tm_map(x, removePunctuation)  # leaving punctuation in case relevant to spam ham filter
x = tm_map(x, stripWhitespace)
x = tm_map(x, stemDocument)
x = tm_map(x, content_transformer(tolower))  # content_transformer required because tolower not tm function
x = tm_map(x, removeWords, stopwords('en'))
}
spam_path = '20021010_spam/'
# add spam to email corpus
# create initial corpus
email = readLines(str_c(spam_path, list.files(spam_path)[1]))
email = cleanse_emails(email)
email_corpus = Corpus(VectorSource(email))
email_corpus = cleanse_corpus(email_corpus)
meta(email_corpus[[1]], 'spam') = 1
cleanse_emails = function(dat) {
dat = str_c(dat, collapse = " ")
dat2 <- unlist(strsplit(dat, split=" "))
dat3 <- grep("dat2", iconv(dat2, "latin1", "ASCII", sub="dat2"))
dat4 = ifelse(length(dat3) == 0, dat2, dat2[-dat3])
# if (length(dat3) == 0) {
#   dat4 = dat2
# } else dat4 = dat2[-dat3]
dat5 <- paste(dat4, collapse = " ")
}
email = readLines(str_c(spam_path, list.files(spam_path)[1]))
email = cleanse_emails(email)
spam_path = '20021010_spam/'
email = readLines(str_c(spam_path, list.files(spam_path)[1]))
email = cleanse_emails(email)
email
# create function to cleanse emails
cleanse_emails = function(dat) {
dat = str_c(dat, collapse = " ")
dat2 <- unlist(strsplit(dat, split=" "))
dat3 <- grep("dat2", iconv(dat2, "latin1", "ASCII", sub="dat2"))
if (length(dat3) == 0) {
dat4 = dat2
} else dat4 = dat2[-dat3]
dat5 <- paste(dat4, collapse = " ")
}
cleanse_corpus = function(x) {
x = tm_map(x, PlainTextDocument)  # to make tm package play nice
x = tm_map(x, removeNumbers)
x = tm_map(x, removePunctuation)  # leaving punctuation in case relevant to spam ham filter
x = tm_map(x, stripWhitespace)
x = tm_map(x, stemDocument)
x = tm_map(x, content_transformer(tolower))  # content_transformer required because tolower not tm function
x = tm_map(x, removeWords, stopwords('en'))
}
spam_path = '20021010_spam/'
# add spam to email corpus
# create initial corpus
email = readLines(str_c(spam_path, list.files(spam_path)[1]))
email = cleanse_emails(email)
email_corpus = Corpus(VectorSource(email))
email_corpus = cleanse_corpus(email_corpus)
meta(email_corpus[[1]], 'spam') = 1
for (i in 2:length(list.files(spam_path))) {
email = readLines(str_c(spam_path, list.files(spam_path)[i]))
email = cleanse_emails(email)
# add meta data to house spam classification
if (length(email) != 0) {
tmp_corpus = Corpus(VectorSource(email))
tmp_corpus = cleanse_corpus(tmp_corpus)
meta(tmp_corpus[[1]], 'spam') = 1
email_corpus = c(email_corpus, tmp_corpus)
}
}
ham_path = '20021010_easy_ham/'
# add ham to email corpus
for (i in 1:length(list.files(ham_path))) {
email = readLines(str_c(ham_path, list.files(ham_path)[i]))
email = cleanse_emails(email)
# add meta data to house spam classification
if (length(email) != 0) {
tmp_corpus = Corpus(VectorSource(email))
tmp_corpus = cleanse_corpus(tmp_corpus)
meta(tmp_corpus[[1]], 'spam') = 0
email_corpus = c(email_corpus, tmp_corpus)
}
}
N = length(email_corpus)
rand_index = sample(1:N)
email_corpus = email_corpus[rand_index]
tm_filter(email_corpus[1:100], FUN = function(x) meta(x)[['spam']] == 1)  # check that spam and ham have been randomized
dtm = DocumentTermMatrix(email_corpus)
dtm = removeSparseTerms(dtm, 1-(10/length(email_corpus)))
dtm
spam_labels = unlist(meta(email_corpus, 'spam'))
container = create_container(dtm, labels = spam_labels,
trainSize = 1:round(N*0.8,0), testSize = (round(N*0.8,0)+1):N,
virgin = FALSE)
svm_model = train_model(container, 'SVM')
svm_model = train_model(container, 'SVM')
svm_out = classify_model(container, svm_model)
svm_out = classify_model(container, svm_model)
labels_out = data.frame(
correct_label = spam_labels[(round(N*0.8,0)+1):N],
svm = as.character(svm_out[,1]),
# tree = as.character(tree_out[,1]),
# maxent = as.character(maxent_out[,1]),
stringsAsFactors = FALSE
)
table(labels_out[,1], labels_out[,2])
table(labels_out[,1] == labels_out[,2])
prop.table(table(labels_out[,1] == labels_out[,2]))
cross_validate(container, nfold = 10, algorithm = 'SVM')
cross_validate(container, nfold = 10, algorithm = 'TREE')
svm_out
?classify_model
cross_validate(container, nfold = 5, algorithm = 'MAXENT')
tree_model = train_model(container, 'TREE')
maxent_model = train_model(container, 'MAXENT')
svm_out = classify_model(container, svm_model)
tree_out = classify_model(container, tree_model)
maxent_out = classify_model(container, maxent_model)
head(svm_out)
labels_out = data.frame(
correct_label = spam_labels[(round(N*0.8,0)+1):N],
svm = as.character(svm_out[,1]),
tree = as.character(tree_out[,1]),
maxent = as.character(maxent_out[,1]),
stringsAsFactors = FALSE
)
table(labels_out[,1], labels_out[,2])
table(labels_out[,1], labels_out[,3])
table(labels_out[,1], labels_out[,4])
